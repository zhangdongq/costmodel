#!/usr/bin/env python3
"""
é›†æˆ CostModel - å‚è€ƒ Galvatron çš„æ•´åˆè®¾è®¡

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ•´åˆç¡¬ä»¶é…ç½®ã€é€šä¿¡æ¨¡å‹ã€è®¡ç®—æ¨¡å‹ã€æ˜¾å­˜æ¨¡å‹
2. é¢„æµ‹å®Œæ•´ step æ—¶å»¶
3. é¢„æµ‹æ˜¾å­˜å ç”¨
4. è‡ªåŠ¨é€‰æ‹©ä¼˜åŒ–ç­–ç•¥
5. é…ç½®æ’åºå’ŒéªŒè¯
"""

import json
import os
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple

from .hardware_config import HardwareConfig, GPUSpecs, NetworkTopology, ClusterConfig
from .communication_model import CommunicationModel, CommResult
from .computation_model import ComputationModel, ModelConfig, ComputeMode
from .memory_model import (
    MemoryModel, MemoryBreakdown, ParallelConfig, TrainingConfig,
    ZeROConfig, ActivationCheckpointConfig, OffloadConfig,
    ZeROStage, CheckpointGranularity, AutoMemoryOptimizer
)


@dataclass
class CostModelConfig:
    """CostModel é…ç½®"""
    # ç¡¬ä»¶é…ç½®
    hardware: HardwareConfig = field(default_factory=HardwareConfig)
    
    # æ¨¡å‹é…ç½®
    model: ModelConfig = field(default_factory=ModelConfig)
    
    # è®­ç»ƒé…ç½®
    training: TrainingConfig = field(default_factory=TrainingConfig)
    
    @classmethod
    def from_dict(cls, data: Dict) -> "CostModelConfig":
        """ä»å­—å…¸åˆ›å»ºé…ç½®"""
        config = cls()
        
        if "hardware" in data:
            hw = data["hardware"]
            config.hardware = HardwareConfig(
                gpu=GPUSpecs(
                    memory_gb=hw.get("gpu_memory_gb", 80.0),
                    bf16_tflops=hw.get("bf16_tflops", 989.0),
                ),
                network=NetworkTopology(
                    intra_node_bandwidth_gbps=hw.get("intra_node_bandwidth_gbps", 900.0),
                    inter_node_bandwidth_gbps=hw.get("inter_node_bandwidth_gbps", 200.0),
                ),
                cluster=ClusterConfig(
                    num_nodes=hw.get("num_nodes", 1),
                    gpus_per_node=hw.get("gpus_per_node", 8),
                ),
            )
        
        if "model" in data:
            m = data["model"]
            config.model = ModelConfig(
                num_layers=m.get("num_layers", 48),
                hidden_size=m.get("hidden_size", 6144),
                intermediate_size=m.get("intermediate_size", 16384),
                num_attention_heads=m.get("num_attention_heads", 32),
                num_key_value_heads=m.get("num_key_value_heads", 4),
                num_experts=m.get("num_experts", 128),
                moe_top_k=m.get("moe_top_k", 8),
                num_moe_layers=m.get("num_moe_layers", 24),
                vocab_size=m.get("vocab_size", 152064),
            )
        
        if "training" in data:
            t = data["training"]
            config.training = TrainingConfig(
                micro_batch_size=t.get("micro_batch_size", 1),
                sequence_length=t.get("sequence_length", 8192),
                global_batch_size=t.get("global_batch_size", 512),
                gradient_accumulation_steps=t.get("gradient_accumulation_steps", 64),
            )
        
        return config
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "hardware": {
                "gpu_memory_gb": self.hardware.gpu.memory_gb,
                "bf16_tflops": self.hardware.gpu.bf16_tflops,
                "intra_node_bandwidth_gbps": self.hardware.network.intra_node_bandwidth_gbps,
                "inter_node_bandwidth_gbps": self.hardware.network.inter_node_bandwidth_gbps,
                "num_nodes": self.hardware.cluster.num_nodes,
                "gpus_per_node": self.hardware.cluster.gpus_per_node,
            },
            "model": {
                "num_layers": self.model.num_layers,
                "hidden_size": self.model.hidden_size,
                "intermediate_size": self.model.intermediate_size,
                "num_attention_heads": self.model.num_attention_heads,
                "num_key_value_heads": self.model.num_key_value_heads,
                "num_experts": self.model.num_experts,
                "moe_top_k": self.model.moe_top_k,
                "num_moe_layers": self.model.num_moe_layers,
                "vocab_size": self.model.vocab_size,
            },
            "training": {
                "micro_batch_size": self.training.micro_batch_size,
                "sequence_length": self.training.sequence_length,
                "global_batch_size": self.training.global_batch_size,
                "gradient_accumulation_steps": self.training.gradient_accumulation_steps,
            },
        }


@dataclass
class PredictionResult:
    """é¢„æµ‹ç»“æœ"""
    # æ—¶å»¶é¢„æµ‹ (ms)
    total_step_time_ms: float = 0.0
    compute_time_ms: float = 0.0
    forward_time_ms: float = 0.0
    backward_time_ms: float = 0.0
    
    # é€šä¿¡æ—¶å»¶ (ms)
    tp_comm_time_ms: float = 0.0
    dp_comm_time_ms: float = 0.0
    ep_comm_time_ms: float = 0.0
    pp_comm_time_ms: float = 0.0
    
    # æµæ°´çº¿
    bubble_time_ms: float = 0.0
    bubble_ratio: float = 0.0
    
    # æ˜¾å­˜ (GB)
    memory_breakdown: MemoryBreakdown = field(default_factory=MemoryBreakdown)
    fits_memory: bool = True
    
    # ä¼˜åŒ–é…ç½®
    zero_config: ZeROConfig = field(default_factory=ZeROConfig)
    checkpoint_config: ActivationCheckpointConfig = field(default_factory=ActivationCheckpointConfig)
    offload_config: OffloadConfig = field(default_factory=OffloadConfig)
    
    # æ•ˆç‡æŒ‡æ ‡
    compute_efficiency: float = 0.0
    mfu: float = 0.0  # Model FLOPs Utilization
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "time": {
                "total_step_time_ms": self.total_step_time_ms,
                "compute_time_ms": self.compute_time_ms,
                "forward_time_ms": self.forward_time_ms,
                "backward_time_ms": self.backward_time_ms,
                "tp_comm_time_ms": self.tp_comm_time_ms,
                "dp_comm_time_ms": self.dp_comm_time_ms,
                "ep_comm_time_ms": self.ep_comm_time_ms,
                "pp_comm_time_ms": self.pp_comm_time_ms,
                "bubble_time_ms": self.bubble_time_ms,
                "bubble_ratio": self.bubble_ratio,
            },
            "memory": self.memory_breakdown.to_dict(),
            "fits_memory": self.fits_memory,
            "optimization": {
                "zero_stage": self.zero_config.stage.value,
                "checkpoint_granularity": self.checkpoint_config.granularity.value,
                "optimizer_offload": self.offload_config.optimizer_offload.value,
            },
            "efficiency": {
                "compute_efficiency": self.compute_efficiency,
                "mfu": self.mfu,
            },
        }


class GalvatronCostModel:
    """
    Galvatron é£æ ¼çš„é›†æˆ CostModel
    
    æ•´åˆï¼š
    - ç¡¬ä»¶é…ç½®ï¼ˆGPUã€ç½‘ç»œã€é›†ç¾¤ï¼‰
    - é€šä¿¡æ¨¡å‹ï¼ˆTP/DP/EP/PP é€šä¿¡ï¼‰
    - è®¡ç®—æ¨¡å‹ï¼ˆÎ±-Î² æ¨¡å‹ï¼‰
    - æ˜¾å­˜æ¨¡å‹ï¼ˆç²¾ç¡®åˆ†è§£ï¼‰
    
    åŠŸèƒ½ï¼š
    - é¢„æµ‹ step æ—¶å»¶
    - é¢„æµ‹æ˜¾å­˜å ç”¨
    - è‡ªåŠ¨é€‰æ‹©ä¼˜åŒ–ç­–ç•¥
    - é…ç½®æ’åºå’ŒéªŒè¯
    """
    
    def __init__(self, config: CostModelConfig):
        self.config = config
        
        # åˆå§‹åŒ–å­æ¨¡å‹
        self.comm_model = CommunicationModel(config.hardware)
        self.compute_model = ComputationModel(
            config.hardware, config.model, ComputeMode.LINEAR
        )
        self.memory_model = MemoryModel(config.model, config.training)
        
        # è‡ªåŠ¨æ˜¾å­˜ä¼˜åŒ–å™¨
        self.memory_optimizer = AutoMemoryOptimizer(
            self.memory_model, config.hardware.gpu.memory_gb
        )
        
        # æ ¡å‡†æ•°æ®
        self.calibration_data: Dict = {}
    
    def predict_step_time(self, parallel: ParallelConfig,
                          micro_batch_size: int = None,
                          sequence_length: int = None,
                          num_micro_batches: int = None) -> Dict[str, float]:
        """
        é¢„æµ‹å•æ­¥è®­ç»ƒæ—¶å»¶
        
        Args:
            parallel: å¹¶è¡Œé…ç½®
            micro_batch_size: micro batch sizeï¼ˆé»˜è®¤ä½¿ç”¨è®­ç»ƒé…ç½®ï¼‰
            sequence_length: åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ä½¿ç”¨è®­ç»ƒé…ç½®ï¼‰
            num_micro_batches: micro batch æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨ gradient_accumulation_stepsï¼‰
        
        Returns:
            æ—¶å»¶è¯¦æƒ…å­—å…¸
        """
        if micro_batch_size is None:
            micro_batch_size = self.config.training.micro_batch_size
        if sequence_length is None:
            sequence_length = self.config.training.sequence_length
        if num_micro_batches is None:
            num_micro_batches = self.config.training.gradient_accumulation_steps
        
        # ========== è®¡ç®—æ—¶é—´ ==========
        forward_time = self.compute_model.estimate_forward_time(
            micro_batch_size, sequence_length,
            parallel.tp_degree, parallel.pp_degree, parallel.ep_degree
        )
        backward_time = self.compute_model.estimate_backward_time(
            micro_batch_size, sequence_length,
            parallel.tp_degree, parallel.pp_degree, parallel.ep_degree
        )
        
        # æµæ°´çº¿æ°”æ³¡
        bubble_time = self.compute_model.estimate_pipeline_bubble_time(
            forward_time, backward_time,
            parallel.pp_degree, num_micro_batches
        )
        
        compute_time = (forward_time + backward_time) * num_micro_batches + bubble_time
        
        # ========== é€šä¿¡æ—¶é—´ ==========
        h = self.config.model.hidden_size
        activation_size = micro_batch_size * sequence_length * h * self.config.training.dtype_bytes
        
        # TP AllReduce (æ¯å±‚ 2 æ¬¡)
        layers_per_stage = self.config.model.num_layers // parallel.pp_degree
        tp_comm_result = self.comm_model.predict_tp_comm(activation_size, parallel.tp_degree)
        tp_comm_time = tp_comm_result.time_ms * 2 * layers_per_stage * num_micro_batches
        
        # EP AllToAll (MoE å±‚)
        moe_layers = self.config.model.num_moe_layers // parallel.pp_degree
        token_data_size = micro_batch_size * sequence_length * h * self.config.model.moe_top_k * self.config.training.dtype_bytes
        ep_comm_result = self.comm_model.predict_ep_comm(
            token_data_size, parallel.ep_degree,
            topk=self.config.model.moe_top_k,
            num_experts=self.config.model.num_experts
        )
        ep_comm_time = ep_comm_result.time_ms * moe_layers * num_micro_batches
        
        # PP P2P
        pp_comm_result = self.comm_model.predict_pp_comm(
            activation_size, parallel.pp_degree, num_micro_batches
        )
        pp_comm_time = pp_comm_result.time_ms
        
        # DP AllReduce/ReduceScatter (æ¢¯åº¦åŒæ­¥)
        param_count = self.memory_model.estimate_parameter_count(parallel)
        grad_size = param_count["total_params"] * self.config.training.dtype_bytes
        dp_comm_result = self.comm_model.predict_dp_comm(
            grad_size, parallel.dp_degree, use_sharding=False
        )
        dp_comm_time = dp_comm_result.time_ms
        
        # ========== æ€»æ—¶å»¶ ==========
        # é€šä¿¡ä¸è®¡ç®—çš„é‡å 
        # TP é€šä¿¡åœ¨å…³é”®è·¯å¾„ä¸Šï¼Œæ— æ³•å®Œå…¨ overlap
        # DP é€šä¿¡å¯ä»¥ä¸åå‘è®¡ç®—éƒ¨åˆ† overlap
        # EP é€šä¿¡åœ¨å…³é”®è·¯å¾„ä¸Š
        
        overlap_factor = 0.3  # å‡è®¾ 30% çš„é€šä¿¡å¯ä»¥ overlap
        effective_comm_time = (
            tp_comm_time +
            ep_comm_time +
            pp_comm_time +
            dp_comm_time * (1 - overlap_factor)
        )
        
        total_time = compute_time + effective_comm_time
        
        # æ°”æ³¡æ¯”ä¾‹
        bubble_ratio = bubble_time / total_time if total_time > 0 else 0
        
        return {
            "total_step_time_ms": total_time,
            "compute_time_ms": compute_time,
            "forward_time_ms": forward_time * num_micro_batches,
            "backward_time_ms": backward_time * num_micro_batches,
            "tp_comm_time_ms": tp_comm_time,
            "dp_comm_time_ms": dp_comm_time,
            "ep_comm_time_ms": ep_comm_time,
            "pp_comm_time_ms": pp_comm_time,
            "bubble_time_ms": bubble_time,
            "bubble_ratio": bubble_ratio,
        }
    
    def predict_memory(self, parallel: ParallelConfig,
                       zero_config: ZeROConfig = None,
                       checkpoint_config: ActivationCheckpointConfig = None,
                       offload_config: OffloadConfig = None) -> MemoryBreakdown:
        """é¢„æµ‹æ˜¾å­˜å ç”¨"""
        return self.memory_model.estimate_memory(
            parallel, zero_config, checkpoint_config, offload_config
        )
    
    def predict_full(self, parallel: ParallelConfig,
                     micro_batch_size: int = None,
                     sequence_length: int = None,
                     auto_optimize: bool = True) -> PredictionResult:
        """
        å®Œæ•´é¢„æµ‹ï¼šæ—¶å»¶ + æ˜¾å­˜ + è‡ªåŠ¨ä¼˜åŒ–
        
        Args:
            parallel: å¹¶è¡Œé…ç½®
            micro_batch_size: micro batch size
            sequence_length: åºåˆ—é•¿åº¦
            auto_optimize: æ˜¯å¦è‡ªåŠ¨é€‰æ‹©ä¼˜åŒ–ç­–ç•¥
        """
        if micro_batch_size is None:
            micro_batch_size = self.config.training.micro_batch_size
        if sequence_length is None:
            sequence_length = self.config.training.sequence_length
        
        result = PredictionResult()
        
        # è‡ªåŠ¨é€‰æ‹©ä¼˜åŒ–ç­–ç•¥
        if auto_optimize:
            zero_cfg, ckpt_cfg, offload_cfg, breakdown = self.memory_optimizer.find_optimal_config(parallel)
            result.zero_config = zero_cfg
            result.checkpoint_config = ckpt_cfg
            result.offload_config = offload_cfg
            result.memory_breakdown = breakdown
        else:
            zero_cfg = ZeROConfig()
            ckpt_cfg = ActivationCheckpointConfig()
            offload_cfg = OffloadConfig()
            result.memory_breakdown = self.predict_memory(parallel, zero_cfg, ckpt_cfg, offload_cfg)
            result.zero_config = zero_cfg
            result.checkpoint_config = ckpt_cfg
            result.offload_config = offload_cfg
        
        # æ£€æŸ¥æ˜¾å­˜
        result.fits_memory = result.memory_breakdown.total_memory_gb <= self.config.hardware.gpu.memory_gb
        
        # é¢„æµ‹æ—¶å»¶
        time_pred = self.predict_step_time(parallel, micro_batch_size, sequence_length)
        
        # è€ƒè™‘ recompute å¼€é”€
        recompute_overhead = result.checkpoint_config.get_recompute_overhead()
        
        result.total_step_time_ms = time_pred["total_step_time_ms"] * recompute_overhead
        result.compute_time_ms = time_pred["compute_time_ms"] * recompute_overhead
        result.forward_time_ms = time_pred["forward_time_ms"] * recompute_overhead
        result.backward_time_ms = time_pred["backward_time_ms"] * recompute_overhead
        result.tp_comm_time_ms = time_pred["tp_comm_time_ms"]
        result.dp_comm_time_ms = time_pred["dp_comm_time_ms"]
        result.ep_comm_time_ms = time_pred["ep_comm_time_ms"]
        result.pp_comm_time_ms = time_pred["pp_comm_time_ms"]
        result.bubble_time_ms = time_pred["bubble_time_ms"]
        result.bubble_ratio = time_pred["bubble_ratio"]
        
        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        result.compute_efficiency = self._calculate_compute_efficiency(result, parallel)
        result.mfu = self._calculate_mfu(result, parallel, micro_batch_size, sequence_length)
        
        return result
    
    def _calculate_compute_efficiency(self, result: PredictionResult,
                                      parallel: ParallelConfig) -> float:
        """è®¡ç®—ç¡¬ä»¶åˆ©ç”¨æ•ˆç‡"""
        if result.total_step_time_ms <= 0:
            return 0.0
        
        # è®¡ç®—æ—¶é—´å æ€»æ—¶é—´çš„æ¯”ä¾‹
        compute_ratio = result.compute_time_ms / result.total_step_time_ms
        
        # è€ƒè™‘å¹¶è¡Œæ•ˆç‡æŸå¤±
        tp_efficiency = 0.9 if parallel.tp_degree > 1 else 1.0
        pp_efficiency = 1.0 - result.bubble_ratio
        ep_efficiency = 0.85 if parallel.ep_degree > 1 else 1.0
        
        return compute_ratio * tp_efficiency * pp_efficiency * ep_efficiency
    
    def _calculate_mfu(self, result: PredictionResult,
                       parallel: ParallelConfig,
                       micro_batch_size: int,
                       sequence_length: int) -> float:
        """
        è®¡ç®— Model FLOPs Utilization (MFU)
        
        MFU = å®é™…ååé‡ / ç†è®ºå³°å€¼ååé‡
        
        å‚è€ƒ Megatron-LM çš„ MFU è®¡ç®—æ–¹å¼
        """
        if result.total_step_time_ms <= 0:
            return 0.0
        
        # ä¼°ç®—æ¨¡å‹ FLOPs (per token)
        h = self.config.model.hidden_size
        ffn = self.config.model.intermediate_size
        num_layers = self.config.model.num_layers
        vocab = self.config.model.vocab_size
        
        # æ¯ä¸ª token çš„ FLOPsï¼ˆå‰å‘ä¼ æ’­ï¼‰
        # Attention: 4 * h^2 (Q,K,V,O) + 2 * seq * h (attention scores, è¿‘ä¼¼)
        # MLP: 8 * h * ffn (SwiGLU: gate, up, down)
        # å¯¹äº MoE å±‚ï¼Œåªæœ‰ topk ä¸ªä¸“å®¶è¢«æ¿€æ´»
        
        # ç®€åŒ–å…¬å¼ï¼šæ¯ token çº¦ 6 * num_params FLOPsï¼ˆå‰å‘ï¼‰
        # å‚è€ƒ: https://arxiv.org/abs/2104.04473
        params_per_layer = 12 * h * h + 8 * h * ffn  # ç²—ç•¥ä¼°ç®—
        flops_per_token_forward = 2 * params_per_layer * num_layers
        
        # æ€» tokens
        tokens = micro_batch_size * sequence_length
        num_micro_batches = self.config.training.gradient_accumulation_steps
        total_tokens = tokens * num_micro_batches
        
        # æ€» FLOPs (å‰å‘ + åå‘ â‰ˆ 3x å‰å‘)
        total_flops = flops_per_token_forward * total_tokens * 3
        
        # å®é™… TFLOPSï¼ˆå•å¡ï¼‰
        actual_tflops = total_flops / (result.total_step_time_ms / 1000) / 1e12
        
        # å³°å€¼ TFLOPSï¼ˆå•å¡ç†è®ºå³°å€¼ï¼‰
        peak_tflops = self.config.hardware.gpu.get_effective_tflops("bf16")
        
        # è®¡ç®— MFUï¼ˆè€ƒè™‘æ‰€æœ‰ GPU ä¸€èµ·å·¥ä½œï¼‰
        world_size = parallel.dp_degree * parallel.tp_degree * parallel.pp_degree
        
        # MFU = å®é™…å•å¡ TFLOPS / å³°å€¼å•å¡ TFLOPS
        # æ³¨æ„ï¼štotal_flops æ˜¯æ€»å·¥ä½œé‡ï¼Œåˆ†æ‘Šåˆ°æ¯å¡åé™¤ä»¥å³°å€¼
        mfu = (actual_tflops / world_size) / peak_tflops if peak_tflops > 0 else 0.0
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        return min(mfu, 1.0)
    
    def rank_configurations(self, configs: List[Dict], top_k: int = 10) -> List[Dict]:
        """
        å¯¹å¹¶è¡Œé…ç½®åˆ—è¡¨è¿›è¡Œæ’åº
        
        æ’åºä¾æ®ï¼š
        1. æ˜¯å¦æ»¡è¶³æ˜¾å­˜çº¦æŸ
        2. step æ—¶å»¶
        3. ç¡¬ä»¶åˆ©ç”¨ç‡
        """
        results = []
        
        for i, cfg in enumerate(configs):
            try:
                parallel = ParallelConfig(
                    dp_degree=cfg.get("dp_degree", 1),
                    tp_degree=cfg.get("tp_degree", 1),
                    pp_degree=cfg.get("pp_degree", 1),
                    ep_degree=cfg.get("ep_degree", 1),
                )
                
                micro_bsz = cfg.get("micro_batch_size", self.config.training.micro_batch_size)
                seq_len = cfg.get("sequence_length", self.config.training.sequence_length)
                
                prediction = self.predict_full(parallel, micro_bsz, seq_len)
                
                results.append({
                    "rank": 0,
                    "config": cfg,
                    "prediction": prediction.to_dict(),
                    "total_step_time_ms": prediction.total_step_time_ms,
                    "total_memory_gb": prediction.memory_breakdown.total_memory_gb,
                    "fits_memory": prediction.fits_memory,
                    "compute_efficiency": prediction.compute_efficiency,
                    "mfu": prediction.mfu,
                })
            except Exception as e:
                print(f"Warning: Failed to predict config {cfg}: {e}")
                continue
        
        # æ’åºï¼šå…ˆæŒ‰ fits_memoryï¼Œå†æŒ‰æ—¶å»¶
        results.sort(key=lambda x: (not x["fits_memory"], x["total_step_time_ms"]))
        
        # æ›´æ–°æ’å
        for i, r in enumerate(results):
            r["rank"] = i + 1
        
        # æ‰“å°æŠ¥å‘Š
        self._print_ranking_report(results[:top_k])
        
        return results[:top_k]
    
    def _print_ranking_report(self, results: List[Dict]):
        """æ‰“å°æ’åºæŠ¥å‘Š"""
        if not results:
            return
        
        print("\n" + "=" * 130)
        print("ğŸš€ Galvatron CostModel - å¹¶è¡Œé…ç½®æ’åºæŠ¥å‘Š")
        print("=" * 130)
        print(f"{'æ’å':<4} {'å¹¶è¡Œé…ç½®':<25} {'æ—¶å»¶(ms)':<12} {'æ˜¾å­˜(GB)':<12} "
              f"{'æ»¡è¶³çº¦æŸ':<10} {'æ•ˆç‡':<8} {'MFU':<8}")
        print("-" * 130)
        
        for r in results:
            cfg = r["config"]
            config_str = f"DP{cfg.get('dp_degree',1)}-TP{cfg.get('tp_degree',1)}-PP{cfg.get('pp_degree',1)}-EP{cfg.get('ep_degree',1)}"
            fits = "âœ…" if r["fits_memory"] else "âŒ"
            
            print(f"{r['rank']:<4} {config_str:<25} {r['total_step_time_ms']:<12.2f} "
                  f"{r['total_memory_gb']:<12.2f} {fits:<10} "
                  f"{r['compute_efficiency']:<8.1%} {r['mfu']:<8.1%}")
        
        print("-" * 130)
        
        if results:
            best = results[0]
            print(f"\nğŸ“Š æœ€ä¼˜é…ç½®: {best['config']}")
            print(f"   â€¢ é¢„è®¡æ—¶å»¶: {best['total_step_time_ms']:.2f} ms")
            print(f"   â€¢ æ˜¾å­˜å ç”¨: {best['total_memory_gb']:.2f} GB")
            print(f"   â€¢ MFU: {best['mfu']:.1%}")
    
    def calibrate_with_data(self, calibration_data: List[Dict]):
        """
        ä½¿ç”¨å®æµ‹æ•°æ®æ ¡å‡†æ¨¡å‹
        
        calibration_data æ ¼å¼ï¼š
        [
            {
                "parallel_config": {"dp": 1, "tp": 8, "pp": 1, "ep": 8},
                "micro_batch_size": 1,
                "sequence_length": 2048,
                "actual_step_time_ms": 150.0,
                "actual_a2a_time_ms": 20.0,
            },
            ...
        ]
        """
        if not calibration_data:
            return
        
        # æå– A2A æ ¡å‡†æ•°æ®
        a2a_calibration = []
        for data in calibration_data:
            if "actual_a2a_time_ms" in data:
                cfg = data.get("parallel_config", {})
                micro_bsz = data.get("micro_batch_size", 1)
                seq_len = data.get("sequence_length", 2048)
                ep = cfg.get("ep", cfg.get("ep_degree", 1))
                
                h = self.config.model.hidden_size
                topk = self.config.model.moe_top_k
                data_size = micro_bsz * seq_len * h * topk * self.config.training.dtype_bytes
                
                a2a_calibration.append({
                    "data_size_bytes": data_size,
                    "num_gpus": ep,
                    "actual_time_ms": data["actual_a2a_time_ms"],
                })
        
        # æ ¡å‡† A2A æ¨¡å‹
        if a2a_calibration:
            self.comm_model.calibrate_alltoall(a2a_calibration)
        
        self.calibration_data = {"calibration_data": calibration_data}
        print(f"Calibrated with {len(calibration_data)} data points")
    
    def load_calibration(self, calibration_path: str):
        """ä»æ–‡ä»¶åŠ è½½æ ¡å‡†æ•°æ®"""
        if not os.path.exists(calibration_path):
            print(f"Warning: Calibration file not found: {calibration_path}")
            return
        
        with open(calibration_path, 'r') as f:
            data = json.load(f)
        
        if isinstance(data, list):
            self.calibrate_with_data(data)
        elif isinstance(data, dict) and "results" in data:
            # æ”¯æŒä»éªŒè¯æ–‡ä»¶åŠ è½½
            self.calibrate_with_data(data["results"])
    
    def save_config(self, path: str):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        data = self.config.to_dict()
        data["calibration"] = self.calibration_data
        
        os.makedirs(os.path.dirname(path) if os.path.dirname(path) else ".", exist_ok=True)
        with open(path, 'w') as f:
            json.dump(data, f, indent=2)
    
    @classmethod
    def from_config_file(cls, path: str) -> "GalvatronCostModel":
        """ä»é…ç½®æ–‡ä»¶åˆ›å»º CostModel"""
        with open(path, 'r') as f:
            data = json.load(f)
        
        config = CostModelConfig.from_dict(data)
        model = cls(config)
        
        if "calibration" in data:
            model.calibration_data = data["calibration"]
        
        return model


# ==================== ä¾¿æ·å‡½æ•° ====================

def create_qwen3_30b_costmodel(gpu_memory_gb: float = 80.0,
                               num_nodes: int = 1,
                               gpus_per_node: int = 8) -> GalvatronCostModel:
    """
    åˆ›å»º Qwen3-30B-A3B çš„ CostModel
    
    é¢„è®¾é…ç½®ï¼š
    - 48 å±‚ (24 Dense + 24 MoE)
    - hidden_size = 6144
    - intermediate_size = 16384
    - num_experts = 128
    - topk = 8
    """
    config = CostModelConfig(
        hardware=HardwareConfig(
            gpu=GPUSpecs(memory_gb=gpu_memory_gb),
            cluster=ClusterConfig(num_nodes=num_nodes, gpus_per_node=gpus_per_node),
        ),
        model=ModelConfig(
            num_layers=48,
            hidden_size=6144,
            intermediate_size=16384,
            num_attention_heads=32,
            num_key_value_heads=4,
            num_experts=128,
            moe_top_k=8,
            num_moe_layers=24,
            vocab_size=152064,
        ),
        training=TrainingConfig(
            micro_batch_size=1,
            sequence_length=8192,
            gradient_accumulation_steps=64,
        ),
    )
    
    return GalvatronCostModel(config)


# ==================== æµ‹è¯•å‡½æ•° ====================

def test_galvatron_costmodel():
    """æµ‹è¯• Galvatron CostModel"""
    print("=" * 80)
    print("Galvatron CostModel æµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»º CostModel
    cm = create_qwen3_30b_costmodel(gpu_memory_gb=80.0, num_nodes=1, gpus_per_node=8)
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {"dp_degree": 1, "tp_degree": 8, "pp_degree": 1, "ep_degree": 8},
        {"dp_degree": 1, "tp_degree": 4, "pp_degree": 2, "ep_degree": 8},
        {"dp_degree": 2, "tp_degree": 4, "pp_degree": 1, "ep_degree": 4},
        {"dp_degree": 4, "tp_degree": 2, "pp_degree": 1, "ep_degree": 2},
        {"dp_degree": 8, "tp_degree": 1, "pp_degree": 1, "ep_degree": 1},
    ]
    
    print("\nå•é…ç½®é¢„æµ‹æµ‹è¯•:")
    print("-" * 80)
    
    for cfg in test_configs:
        parallel = ParallelConfig(
            dp_degree=cfg["dp_degree"],
            tp_degree=cfg["tp_degree"],
            pp_degree=cfg["pp_degree"],
            ep_degree=cfg["ep_degree"],
        )
        
        result = cm.predict_full(parallel, micro_batch_size=1, sequence_length=2048)
        
        config_str = f"DP{cfg['dp_degree']}-TP{cfg['tp_degree']}-PP{cfg['pp_degree']}-EP{cfg['ep_degree']}"
        fits = "âœ…" if result.fits_memory else "âŒ"
        
        print(f"{config_str:<20} æ—¶å»¶: {result.total_step_time_ms:>8.2f}ms  "
              f"æ˜¾å­˜: {result.memory_breakdown.total_memory_gb:>6.1f}GB {fits}  "
              f"MFU: {result.mfu:>5.1%}")
    
    print("-" * 80)
    
    # é…ç½®æ’åºæµ‹è¯•
    print("\né…ç½®æ’åºæµ‹è¯•:")
    cm.rank_configurations(test_configs, top_k=5)


if __name__ == "__main__":
    test_galvatron_costmodel()