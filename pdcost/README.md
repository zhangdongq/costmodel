# pdcost - PaddleFormers åˆ†å¸ƒå¼è®­ç»ƒä»£ä»·æ¨¡å‹

`pdcost` æ˜¯ä¸€ä¸ªç”¨äºé¢„æµ‹ PaddleFormers åˆ†å¸ƒå¼è®­ç»ƒæ€§èƒ½çš„å·¥å…·ï¼Œå¯ä»¥åœ¨å®é™…è¿è¡Œå‰ä¼°ç®—ä¸åŒå¹¶è¡Œé…ç½®ä¸‹çš„ï¼š
- **Step æ—¶é—´** (è®­ç»ƒè¿­ä»£è€—æ—¶ï¼Œå·²æ ¡å‡† seq_len é˜ˆå€¼æ•ˆåº”)
- **æ˜¾å­˜å ç”¨** (æ”¯æŒåŒæŒ‡æ ‡: allocated + reserved)
- **ç¡¬ä»¶åˆ©ç”¨ç‡** (MFU)
- **è®­ç»ƒååé‡** (tokens/s/GPU)

## âœ¨ ç‰¹æ€§äº®ç‚¹

- ğŸ¯ **é«˜ç²¾åº¦é¢„æµ‹**: Step time è¯¯å·® ~5%ï¼Œæ˜¾å­˜è¯¯å·® ~10%
- ğŸ“Š **åŒæŒ‡æ ‡æ˜¾å­˜**: åŒæ—¶é¢„æµ‹ `allocated` (å®é™…åˆ†é…) å’Œ `reserved` (æ¡†æ¶é¢„ç•™)
- ğŸ”§ **seq_len æ ¡å‡†**: å†…ç½®é˜ˆå€¼æ•ˆåº”æ¨¡å‹ï¼Œå‡†ç¡®å¤„ç†ä¸åŒåºåˆ—é•¿åº¦
- ğŸ” **é…ç½®æœç´¢**: è‡ªåŠ¨æœç´¢æœ€ä¼˜å¹¶è¡Œé…ç½®ï¼Œæ”¯æŒ OOM è¿‡æ»¤
- âš¡ **MoE ä¸“ç”¨**: é’ˆå¯¹ Qwen3 MoE ç­‰ç¨€ç–æ¨¡å‹ä¼˜åŒ–

## ğŸ“¦ æ”¯æŒçš„å¹¶è¡Œç­–ç•¥

| å¹¶è¡Œç­–ç•¥ | å‚æ•° | è¯´æ˜ |
|---------|------|------|
| Tensor Parallel (TP) | `tp` | å¼ é‡å¹¶è¡Œï¼Œåˆ‡åˆ† Attention å’Œ MLP æƒé‡ |
| Pipeline Parallel (PP) | `pp` | æµæ°´çº¿å¹¶è¡Œï¼Œåˆ‡åˆ† Transformer å±‚ |
| Data Parallel (DP) | `dp` | æ•°æ®å¹¶è¡Œï¼Œå¤åˆ¶æ¨¡å‹ |
| Expert Parallel (EP) | `ep` | ä¸“å®¶å¹¶è¡Œï¼Œåˆ‡åˆ† MoE ä¸“å®¶ |
| Sharding (ZeRO) | `sharding` | ä¼˜åŒ–å™¨çŠ¶æ€/æ¢¯åº¦/å‚æ•°åˆ†ç‰‡ |
| Sequence Parallel (SP) | `sp` | åºåˆ—å¹¶è¡Œï¼Œé…åˆ TP ä½¿ç”¨ |
| Context Parallel (CP) | `cp` | ä¸Šä¸‹æ–‡å¹¶è¡Œï¼Œåˆ‡åˆ†åºåˆ—é•¿åº¦ |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€ç”¨æ³•

```python
from pdcost import PDCostModel, ModelConfig, ParallelConfig

# 1. åˆ›å»ºæ¨¡å‹é…ç½® (æ”¯æŒé¢„è®¾æ¨¡å‹)
model_config = ModelConfig.from_name("qwen3-30b-a3b")

# 2. åˆ›å»ºä»£ä»·æ¨¡å‹
costmodel = PDCostModel(model_config)

# 3. é¢„æµ‹å¹¶è¡Œé…ç½®
parallel = ParallelConfig(tp=8, pp=1, dp=1, ep=8, sharding="stage1")
result = costmodel.predict(parallel, micro_batch_size=1, seq_len=8192)

# 4. æŸ¥çœ‹ç»“æœ
print(f"Step Time: {result.step_time_ms:.2f} ms")
print(f"Memory: {result.memory_gb:.2f} GB")
print(f"MFU: {result.mfu:.1%}")
```

### æ¯”è¾ƒå¤šä¸ªé…ç½®

```python
configs = [
    {"tp": 8, "pp": 1, "dp": 1, "ep": 8, "sharding": "stage1"},
    {"tp": 4, "pp": 2, "dp": 1, "ep": 4, "sharding": "stage1"},
    {"tp": 4, "pp": 1, "dp": 2, "ep": 4, "sharding": "stage2"},
]

# è‡ªåŠ¨æ’åºå¹¶è¾“å‡ºæŠ¥å‘Š
best_configs = costmodel.rank_configurations(configs, top_k=5)
```

### è‡ªåŠ¨æœç´¢æœ€ä¼˜é…ç½®

```python
# ç”Ÿæˆæœç´¢ç©ºé—´
configs = costmodel.generate_search_space(total_gpus=16, max_tp=8, max_pp=4)

# æœç´¢æœ€ä¼˜é…ç½®
best_configs = costmodel.rank_configurations(configs, top_k=10)
```

## ğŸ“Š æ”¯æŒçš„é¢„è®¾æ¨¡å‹

| æ¨¡å‹åç§° | ç±»å‹ | å‚æ•°é‡ | è¯´æ˜ |
|---------|------|--------|------|
| `qwen3-30b-a3b` | MoE | ~30B | Qwen3 MoE, 128 experts, top-8 |
| `qwen3-235b-a22b` | MoE | ~235B | Qwen3 å¤§æ¨¡å‹ |
| `deepseek-v3` | MoE | ~685B | DeepSeek V3 |
| `llama3-70b` | Dense | ~70B | LLaMA 3 70B |
| `llama3-8b` | Dense | ~8B | LLaMA 3 8B |

## ğŸ”§ ç¡¬ä»¶æ ¡å‡†

pdcost æ”¯æŒé€šè¿‡å®é™…è¿è¡Œ benchmark æµ‹è¯• GPU ç®—åŠ›å’Œæ˜¾å­˜å¸¦å®½ï¼Œè‡ªåŠ¨æ ¡å‡†ç¡¬ä»¶å‚æ•°ï¼Œæé«˜é¢„æµ‹ç²¾åº¦ã€‚

### å¿«é€Ÿæ ¡å‡†

```python
from pdcost import quick_calibrate

# æ‰§è¡Œæ ¡å‡†
result = quick_calibrate(device_id=0)
print(result)
# CalibrationResult:
#   GPU: NVIDIA H800 Ã— 8
#   Memory: 79.6 GB
#   FP32: 51.6 TFLOPS
#   FP16: 763.0 TFLOPS
#   BF16: 788.0 TFLOPS
#   Memory BW: 2781.8 GB/s
```

### è‡ªåŠ¨æ ¡å‡†åˆ›å»º CostModel

```python
from pdcost import PDCostModel, ModelConfig

model_config = ModelConfig.from_name("qwen3-30b-a3b")

# åˆå§‹åŒ–æ—¶è‡ªåŠ¨æ ¡å‡†
costmodel = PDCostModel(model_config, auto_calibrate=True)

# ä½¿ç”¨æ ¡å‡†åçš„ç¡¬ä»¶å‚æ•°è¿›è¡Œé¢„æµ‹
result = costmodel.predict(parallel)
```

### æ‰‹åŠ¨æ ¡å‡†

```python
costmodel = PDCostModel(model_config)

# æ‰‹åŠ¨è§¦å‘æ ¡å‡† (å¯æŒ‡å®š GEMM çŸ©é˜µå¤§å°åŠ å¿«æµ‹è¯•)
costmodel.calibrate(gemm_size=4096)

# æŸ¥çœ‹æ ¡å‡†ç»“æœ
print(costmodel.calibration_result)
print(f"BF16 ç®—åŠ›: {costmodel.hardware_config.gpu.bf16_tflops:.1f} TFLOPS")
```

### æ ¡å‡†å‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `gemm_size` | GEMM æµ‹è¯•çŸ©é˜µå¤§å° | 8192 |
| `test_compute` | æ˜¯å¦æµ‹è¯•ç®—åŠ› | True |
| `test_memory` | æ˜¯å¦æµ‹è¯•æ˜¾å­˜å¸¦å®½ | True |
| `verbose` | æ˜¯å¦æ‰“å°è¿›åº¦ | True |

## ğŸ“ æ¨¡å—ç»“æ„

```
pdcost/
â”œâ”€â”€ __init__.py          # ä¸»å…¥å£
â”œâ”€â”€ config.py            # é…ç½®ç±» (ModelConfig, ParallelConfig, etc.)
â”œâ”€â”€ memory_model.py      # æ˜¾å­˜é¢„æµ‹æ¨¡å‹
â”œâ”€â”€ compute_model.py     # è®¡ç®—æ—¶é—´é¢„æµ‹æ¨¡å‹
â”œâ”€â”€ comm_model.py        # é€šä¿¡æ—¶é—´é¢„æµ‹æ¨¡å‹
â”œâ”€â”€ calibration.py       # ç¡¬ä»¶æ ¡å‡†æ¨¡å—
â”œâ”€â”€ costmodel.py         # ä¸» CostModel ç±»
â”œâ”€â”€ README.md            # æ–‡æ¡£
â””â”€â”€ examples/
    â””â”€â”€ basic_usage.py   # ä½¿ç”¨ç¤ºä¾‹
```

## ğŸ”§ é…ç½®è¯¦è§£

### ModelConfig - æ¨¡å‹æ¶æ„é…ç½®

```python
ModelConfig(
    num_hidden_layers=48,       # Transformer å±‚æ•°
    hidden_size=6144,           # éšè—ç»´åº¦
    intermediate_size=16384,    # FFN ä¸­é—´ç»´åº¦
    num_attention_heads=32,     # æ³¨æ„åŠ›å¤´æ•°
    num_key_value_heads=4,      # KV å¤´æ•° (GQA)
    head_dim=192,               # æ¯ä¸ªå¤´çš„ç»´åº¦
    num_experts=128,            # MoE ä¸“å®¶æ•°
    num_experts_per_tok=8,      # Top-K
    moe_intermediate_size=1408, # ä¸“å®¶ FFN ç»´åº¦
    vocab_size=152064,          # è¯è¡¨å¤§å°
)
```

### ParallelConfig - å¹¶è¡Œé…ç½®

```python
ParallelConfig(
    tp=8,                # å¼ é‡å¹¶è¡Œåº¦
    pp=1,                # æµæ°´çº¿å¹¶è¡Œåº¦
    dp=1,                # æ•°æ®å¹¶è¡Œåº¦
    ep=8,                # ä¸“å®¶å¹¶è¡Œåº¦
    sharding="stage1",   # ZeRO é˜¶æ®µ: none/stage1/stage2/stage3
    sp=False,            # åºåˆ—å¹¶è¡Œ
    cp=1,                # ä¸Šä¸‹æ–‡å¹¶è¡Œåº¦
)
```

### TrainingConfig - è®­ç»ƒé…ç½®

```python
TrainingConfig(
    micro_batch_size=1,              # æ¯å¡ batch size
    sequence_length=8192,            # åºåˆ—é•¿åº¦
    gradient_accumulation_steps=64,  # æ¢¯åº¦ç´¯ç§¯
    dtype="bfloat16",                # æ•°æ®ç±»å‹
    recompute_granularity="full",    # é‡è®¡ç®—: none/selective/full
)
```

### HardwareConfig - ç¡¬ä»¶é…ç½®

```python
HardwareConfig(
    gpu=GPUSpec.from_name("H100-80GB-HBM3"),  # GPU è§„æ ¼
    num_nodes=1,                               # èŠ‚ç‚¹æ•°
    gpus_per_node=8,                           # æ¯èŠ‚ç‚¹ GPU æ•°
)
```

## ğŸ“ˆ é¢„æµ‹å‡½æ•°å‚æ•°

```python
result = costmodel.predict(
    parallel,                           # ParallelConfig: å¹¶è¡Œé…ç½®
    micro_batch_size=1,                 # æ¯å¡ batch size
    seq_len=8192,                       # åºåˆ—é•¿åº¦
    max_seq_len=8192,                   # æœ€å¤§åºåˆ—é•¿åº¦ (ç”¨äºæ¿€æ´»æ˜¾å­˜ä¼°ç®—)
    gradient_accumulation_steps=64,     # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    recompute_granularity="full",       # é‡è®¡ç®—ç²’åº¦: "none", "selective", "full"
    tensorwise_offload_optimizer=False, # æ˜¯å¦å¯ç”¨ tensorwise ä¼˜åŒ–å™¨ offload
    tensorwise_offload_ratio=0.95,      # offload æ¯”ä¾‹ (é»˜è®¤ 95%)
)
```

### å…³é”®å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | å½±å“ |
|------|------|------|
| `max_seq_len` | æœ€å¤§åºåˆ—é•¿åº¦ | å½±å“æ¿€æ´»æ˜¾å­˜ä¼°ç®—ï¼ˆè€ƒè™‘åŠ¨æ€ batch paddingï¼‰ |
| `recompute_granularity` | é‡è®¡ç®—ç­–ç•¥ | "none" ä¸é‡è®¡ç®—ï¼›"full" å…¨éƒ¨é‡è®¡ç®—ï¼Œæ¿€æ´»æ˜¾å­˜æœ€ä½ |
| `tensorwise_offload_optimizer` | Tensorwise ä¼˜åŒ–å™¨ offload | ä¼˜åŒ–å™¨çŠ¶æ€æŒ‰ tensor ç²’åº¦åŠ¨æ€ offload åˆ° CPU |
| `tensorwise_offload_ratio` | Offload æ¯”ä¾‹ | é»˜è®¤ 0.95ï¼Œå³ 95% çš„ä¼˜åŒ–å™¨çŠ¶æ€å¯è¢« offload |
| `split_param` | ShardingV2 å‚æ•°åˆ†ç‰‡ | Stage1 ä¹Ÿåˆ†ç‰‡å‚æ•°å’Œæ¢¯åº¦ï¼ˆPaddleFormers ç‰¹æœ‰ï¼‰ |
| `sd_release_grads` | é‡Šæ”¾æ¢¯åº¦ä¼˜åŒ– | æ¯æ¬¡è¿­ä»£åé‡Šæ”¾æ¢¯åº¦ï¼Œæ˜¾è‘—é™ä½å³°å€¼æ˜¾å­˜ |

### æ˜¾å­˜ä¼˜åŒ–ç¤ºä¾‹

```python
# ä¸å¯ç”¨ä¼˜åŒ–å™¨ offload (é»˜è®¤)
result = costmodel.predict(parallel)
# ä¼˜åŒ–å™¨æ˜¾å­˜: 36.58 GB

# å¯ç”¨ tensorwise offload (95%)
result = costmodel.predict(parallel, tensorwise_offload_optimizer=True)
# ä¼˜åŒ–å™¨æ˜¾å­˜: 1.83 GB

# ä½¿ç”¨ max_seq_len ä¼°ç®—å³°å€¼æ¿€æ´»æ˜¾å­˜
result = costmodel.predict(parallel, seq_len=4096, max_seq_len=8192)
# æ¿€æ´»æ˜¾å­˜æŒ‰ max_seq_len=8192 ä¼°ç®—
```

## ğŸ“Š é¢„æµ‹ç»“æœ (PredictionResult)

```python
result = costmodel.predict(parallel)

# æ—¶å»¶æŒ‡æ ‡
result.step_time_ms          # æ€» step æ—¶é—´ (ms)
result.compute_time_ms       # è®¡ç®—æ—¶é—´ (ms)
result.total_comm_time_ms    # é€šä¿¡æ—¶é—´ (ms)
result.bubble_time_ms        # æµæ°´çº¿æ°”æ³¡ (ms)

# æ˜¾å­˜æŒ‡æ ‡
result.memory_gb             # æ€»æ˜¾å­˜ (GB)
result.memory_breakdown      # è¯¦ç»†æ˜¾å­˜åˆ†è§£
result.fits_memory           # æ˜¯å¦æ»¡è¶³æ˜¾å­˜çº¦æŸ

# æ•ˆç‡æŒ‡æ ‡
result.mfu                   # Model FLOPs Utilization
result.compute_efficiency    # è®¡ç®—æ•ˆç‡

# ååé‡
result.tokens_per_second     # æ€»ååé‡ (tok/s)
result.tokens_per_second_per_gpu  # æ¯å¡ååé‡
```

## ?? æ˜¾å­˜åˆ†è§£ (MemoryBreakdown)

```python
breakdown = result.memory_breakdown

# ä¸»è¦ç»„æˆ
breakdown.parameter_memory_gb       # å‚æ•°æ˜¾å­˜
breakdown.gradient_memory_gb        # æ¢¯åº¦æ˜¾å­˜
breakdown.optimizer_memory_gb       # ä¼˜åŒ–å™¨çŠ¶æ€æ˜¾å­˜
breakdown.activation_memory_gb      # æ¿€æ´»å€¼æ˜¾å­˜
breakdown.communication_buffer_gb   # é€šä¿¡ç¼“å†²åŒº
breakdown.temporary_buffer_gb       # ä¸´æ—¶ç¼“å†²åŒº
breakdown.framework_overhead_gb     # æ¡†æ¶åŸºç¡€å¼€é”€ (CUDA/Paddle runtime)

# åŒæŒ‡æ ‡æ˜¾å­˜ (PaddleFormers ç‰¹æœ‰)
breakdown.allocated_memory_gb       # å®é™…åˆ†é…æ˜¾å­˜ (nvidia-smi ä¸­çš„ allocated)
breakdown.reserved_memory_gb        # é¢„ç•™æ˜¾å­˜ (nvidia-smi ä¸­çš„ reserved)
breakdown.activation_buffer_pool_gb # æ¡†æ¶æ¿€æ´»ç¼“å†²æ±  (reserved - allocated)
breakdown.total_memory_gb           # æ€»æ˜¾å­˜ (ç­‰äº reserved)
```

### åŒæŒ‡æ ‡æ˜¾å­˜è¯´æ˜

PaddleFormers æ¡†æ¶æœ‰ä¸¤ä¸ªæ˜¾å­˜æŒ‡æ ‡ï¼š
- **allocated**: å®é™…åˆ†é…çš„æ˜¾å­˜ï¼ŒåŒ…æ‹¬å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨ã€æ¿€æ´»ç­‰
- **reserved**: æ¡†æ¶é¢„ç•™çš„æ˜¾å­˜æ± ï¼ŒåŒ…æ‹¬ allocated + æ¿€æ´»ç¼“å†²æ± 

```python
# é¢„æµ‹åŒæŒ‡æ ‡æ˜¾å­˜
result = costmodel.predict_calibrated(parallel, seq_len=8192, ...)
mb = result.memory_breakdown

print(f"é¢„æµ‹ allocated: {mb.allocated_memory_gb:.2f} GB")  # ~43.4 GB
print(f"é¢„æµ‹ reserved: {mb.reserved_memory_gb:.2f} GB")    # ~58.0 GB
```

## ğŸ” é…ç½®æœç´¢

### æœç´¢æœ€ä¼˜ååé‡é…ç½®

```python
from pdcost import ModelConfig, PDCostModel, ParallelConfig
from pdcost.config import TrainingConfig, HardwareConfig, GPUSpec

# åŠ è½½æ¨¡å‹é…ç½®
model = ModelConfig.from_json('Qwen3-30B-A3B-Base/config.json')
hardware = HardwareConfig(
    gpu=GPUSpec(name='H800', memory_gb=79.6, bf16_tflops=788.0),
    num_nodes=1, gpus_per_node=8
)
training = TrainingConfig(micro_batch_size=1, sequence_length=8192, dtype='bfloat16')
costmodel = PDCostModel(model, hardware, training)

# æœç´¢æœ€ä¼˜é…ç½®
best_configs = costmodel.search_best_throughput(
    total_gpus=8,
    seq_len=8192,
    micro_batch_size=1,
    gradient_accumulation_steps=16,
    tensorwise_offload_optimizer=True,
    top_k=5
)

# è¾“å‡ºç»“æœ
for i, cfg in enumerate(best_configs):
    print(f"#{i+1}: {cfg['throughput']:.0f} tok/s/GPU, "
          f"tp={cfg['tp']}, pp={cfg['pp']}, dp={cfg['dp']}, ep={cfg['ep']}")
```

### æœç´¢ç©ºé—´çº¦æŸ

é…ç½®æœç´¢ä¼šè‡ªåŠ¨è¿‡æ»¤æ— æ•ˆé…ç½®ï¼š
- `tp * pp * dp == total_gpus` (GPU æ•°é‡çº¦æŸ)
- `ep <= num_experts` ä¸” `ep` æ•´é™¤ä¸“å®¶æ•°
- æ˜¾å­˜ä¸è¶…è¿‡ GPU å®¹é‡ (OOM è¿‡æ»¤)
- `tensorwise_offload` éœ€è¦ `dp > 1` (Sharding çº¦æŸ)

## ğŸ“‹ å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

### ä» YAML é…ç½®é¢„æµ‹

```python
from pdcost import ModelConfig, PDCostModel, ParallelConfig
from pdcost.config import TrainingConfig, HardwareConfig, GPUSpec

# 1. åŠ è½½æ¨¡å‹é…ç½®
model = ModelConfig.from_json('Qwen3-30B-A3B-Base/config.json')

# 2. é…ç½®ç¡¬ä»¶ (H800 8å¡)
hardware = HardwareConfig(
    gpu=GPUSpec(name='H800', memory_gb=79.6, bf16_tflops=788.0),
    num_nodes=1, 
    gpus_per_node=8
)

# 3. è®­ç»ƒé…ç½®
training = TrainingConfig(
    micro_batch_size=1, 
    sequence_length=8192, 
    dtype='bfloat16'
)

# 4. åˆ›å»ºä»£ä»·æ¨¡å‹
costmodel = PDCostModel(model, hardware, training)

# 5. é…ç½®å¯¹åº” benchmark_config.yaml
# tp=1, pp=1, dp=8, ep=8, seq=8192, mbs=1, gas=16
parallel = ParallelConfig(tp=1, pp=1, dp=8, ep=8, sharding='stage1')

# 6. é¢„æµ‹ (ä½¿ç”¨æ ¡å‡†åçš„æ¨¡å‹)
result = costmodel.predict_calibrated(
    parallel, 
    seq_len=8192, 
    micro_batch_size=1, 
    gradient_accumulation_steps=16,
    tensorwise_offload_optimizer=True, 
    tensorwise_offload_ratio=0.95
)

# 7. è¾“å‡ºç»“æœ
print(f"Step Time: {result.step_time_ms/1000:.2f} ç§’")
print(f"ååé‡: {result.tokens_per_second_per_gpu:.0f} tok/s/GPU")
print(f"Allocated æ˜¾å­˜: {result.memory_breakdown.allocated_memory_gb:.2f} GB")
print(f"Reserved æ˜¾å­˜: {result.memory_breakdown.reserved_memory_gb:.2f} GB")
print(f"å¯è¿è¡Œ: {'âœ…' if result.fits_memory else 'âŒ OOM'}")
```

### é¢„æµ‹ç²¾åº¦å‚è€ƒ

åœ¨ Qwen3-30B-A3B + H800 8å¡ç¯å¢ƒä¸‹çš„é¢„æµ‹ç²¾åº¦ï¼š

| æŒ‡æ ‡ | é¢„æµ‹è¯¯å·® |
|------|----------|
| Step Time | ~5% |
| ååé‡ (tok/s/GPU) | ~5% |
| Allocated æ˜¾å­˜ | ~10% |
| Reserved æ˜¾å­˜ | ~8% |

### seq_len é˜ˆå€¼æ•ˆåº”

pdcost å†…ç½®äº† seq_len å¯¹ step time çš„é˜ˆå€¼æ•ˆåº”æ ¡å‡†ï¼š

```python
# seq_len <= 2048: åŸºç¡€æ•ˆç‡ ~15%
# seq_len > 2048: æ•ˆç‡éš seq_len çº¿æ€§å¢é•¿
# ä¾‹å¦‚ seq_len=8192 æ—¶æ•ˆç‡å¯è¾¾ ~60%

# è¿™ä¸ªæ•ˆåº”ä¼šè‡ªåŠ¨åœ¨ predict_calibrated() ä¸­è€ƒè™‘
result = costmodel.predict_calibrated(parallel, seq_len=8192, ...)
```

## ğŸ¯ ä½¿ç”¨å»ºè®®

1. **MoE æ¨¡å‹**: ä¼˜å…ˆä½¿ç”¨ EP å¹¶è¡Œï¼Œé€šå¸¸ `ep = min(num_experts, total_gpus)`
2. **æ˜¾å­˜ä¸è¶³**: å°è¯•å¢åŠ  Sharding é˜¶æ®µ (`stage2` â†’ `stage3`) æˆ–å¼€å¯é‡è®¡ç®—
3. **å¤§åºåˆ—é•¿åº¦**: è€ƒè™‘ä½¿ç”¨ Context Parallel (CP) æˆ– Sequence Parallel (SP)
4. **å¤šèŠ‚ç‚¹è®­ç»ƒ**: PP é€‚åˆè·¨èŠ‚ç‚¹ï¼ŒTP å»ºè®®èŠ‚ç‚¹å†…ä½¿ç”¨

## ğŸ“ è¿è¡Œç¤ºä¾‹

```bash
cd pdcost
python examples/basic_usage.py
```

## âš ï¸ æ³¨æ„äº‹é¡¹

- é¢„æµ‹ç»“æœä¸ºç†è®ºä¼°ç®—å€¼ï¼Œå®é™…æ€§èƒ½å—å¤šç§å› ç´ å½±å“
- å»ºè®®åœ¨å°‘é‡é…ç½®ä¸Šè¿›è¡Œå®é™… benchmark éªŒè¯
- é€šä¿¡æ—¶é—´é¢„æµ‹å‡è®¾ç†æƒ³çš„ç½‘ç»œæ¡ä»¶
- **å»ºè®®ä½¿ç”¨ `auto_calibrate=True` è·å–æ›´å‡†ç¡®çš„ç¡¬ä»¶å‚æ•°**

## ğŸ“– API å‚è€ƒ

### PDCostModel

```python
PDCostModel(
    model_config,           # ModelConfig: æ¨¡å‹æ¶æ„é…ç½®
    hardware_config=None,   # HardwareConfig: ç¡¬ä»¶é…ç½® (é»˜è®¤ H100-80GB)
    training_config=None,   # TrainingConfig: è®­ç»ƒé…ç½®
    auto_calibrate=False,   # bool: æ˜¯å¦è‡ªåŠ¨æ ¡å‡†ç¡¬ä»¶
)
```

**ä¸»è¦æ–¹æ³•:**
- `predict(parallel, ...)` - é¢„æµ‹å¹¶è¡Œé…ç½®æ€§èƒ½
- `calibrate(...)` - æ‰§è¡Œç¡¬ä»¶æ ¡å‡†
- `rank_configurations(configs, ...)` - é…ç½®æ’åº
- `generate_search_space(total_gpus, ...)` - ç”Ÿæˆæœç´¢ç©ºé—´

### HardwareCalibrator

```python
from pdcost import HardwareCalibrator

calibrator = HardwareCalibrator(device_id=0)
result = calibrator.calibrate()
hw_config = calibrator.create_hardware_config()
```

### ä¾¿æ·å‡½æ•°

```python
from pdcost import quick_calibrate, create_calibrated_hardware_config

# å¿«é€Ÿæ ¡å‡†
result = quick_calibrate()

# åˆ›å»ºæ ¡å‡†åçš„ç¡¬ä»¶é…ç½®
hw_config = create_calibrated_hardware_config(num_nodes=1, gpus_per_node=8)
```