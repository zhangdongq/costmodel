#!/usr/bin/env python3
"""
pdcost åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ pdcost é¢„æµ‹ PaddleFormers åˆ†å¸ƒå¼è®­ç»ƒçš„æ€§èƒ½
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from pdcost import (
    PDCostModel, 
    ModelConfig, 
    ParallelConfig, 
    TrainingConfig,
    HardwareConfig,
    GPUSpec,
)


def example_single_config():
    """ç¤ºä¾‹ 1: é¢„æµ‹å•ä¸ªå¹¶è¡Œé…ç½®"""
    print("\n" + "=" * 80)
    print("ğŸ“Œ ç¤ºä¾‹ 1: é¢„æµ‹å•ä¸ªå¹¶è¡Œé…ç½®")
    print("=" * 80)
    
    # 1. åˆ›å»ºæ¨¡å‹é…ç½® (Qwen3-30B-A3B MoE)
    model_config = ModelConfig.from_name("qwen3-30b-a3b")
    print(f"\næ¨¡å‹: Qwen3-30B-A3B")
    params = model_config.estimate_parameters()
    print(f"å‚æ•°é‡: {params['total_billion']:.2f}B")
    print(f"  - Embedding: {params['embedding']/1e9:.2f}B")
    print(f"  - Attention: {params['attention']/1e9:.2f}B")
    print(f"  - Dense MLP: {params['dense_mlp']/1e9:.2f}B")
    print(f"  - MoE: {params['moe']/1e9:.2f}B")
    
    # 2. åˆ›å»ºç¡¬ä»¶é…ç½® (8x H100)
    hardware_config = HardwareConfig(
        gpu=GPUSpec.from_name("H100-80GB-HBM3"),
        num_nodes=1,
        gpus_per_node=8,
    )
    print(f"\nç¡¬ä»¶: {hardware_config.gpu.name} Ã— {hardware_config.total_gpus}")
    
    # 3. åˆ›å»ºè®­ç»ƒé…ç½®
    training_config = TrainingConfig(
        micro_batch_size=1,
        sequence_length=8192,
        gradient_accumulation_steps=64,
        dtype="bfloat16",
        recompute_granularity="full",
    )
    print(f"è®­ç»ƒé…ç½®: mbs={training_config.micro_batch_size}, "
          f"seq={training_config.sequence_length}, "
          f"grad_acc={training_config.gradient_accumulation_steps}")
    
    # 4. åˆ›å»º CostModel
    costmodel = PDCostModel(model_config, hardware_config, training_config)
    
    # 5. é¢„æµ‹å¹¶è¡Œé…ç½®
    parallel = ParallelConfig(
        tp=8,
        pp=1,
        dp=1,
        ep=8,
        sharding="stage1",
    )
    print(f"\nå¹¶è¡Œé…ç½®: {parallel}")
    
    result = costmodel.predict(parallel)
    print(f"\né¢„æµ‹ç»“æœ:")
    print(result)
    
    # è¯¦ç»†æ˜¾å­˜åˆ†è§£
    print(f"\næ˜¾å­˜è¯¦æƒ…:")
    print(result.memory_breakdown)


def example_compare_configs():
    """ç¤ºä¾‹ 2: æ¯”è¾ƒå¤šä¸ªå¹¶è¡Œé…ç½®"""
    print("\n" + "=" * 80)
    print("ğŸ“Œ ç¤ºä¾‹ 2: æ¯”è¾ƒå¤šä¸ªå¹¶è¡Œé…ç½®")
    print("=" * 80)
    
    model_config = ModelConfig.from_name("qwen3-30b-a3b")
    costmodel = PDCostModel(model_config)
    
    # å®šä¹‰å¤šä¸ªå¾…æ¯”è¾ƒçš„é…ç½®
    configs = [
        {"tp": 1, "pp": 1, "dp": 8, "ep": 1, "sharding": "stage2"},
        {"tp": 2, "pp": 1, "dp": 4, "ep": 2, "sharding": "stage1"},
        {"tp": 4, "pp": 1, "dp": 2, "ep": 4, "sharding": "stage1"},
        {"tp": 8, "pp": 1, "dp": 1, "ep": 8, "sharding": "stage1"},
        {"tp": 4, "pp": 2, "dp": 1, "ep": 4, "sharding": "stage1"},
    ]
    
    # æ‰¹é‡é¢„æµ‹å¹¶æ’åº
    results = costmodel.rank_configurations(configs, top_k=5)
    
    return results


def example_search_space():
    """ç¤ºä¾‹ 3: æœç´¢æœ€ä¼˜é…ç½®"""
    print("\n" + "=" * 80)
    print("ğŸ“Œ ç¤ºä¾‹ 3: è‡ªåŠ¨æœç´¢æœ€ä¼˜é…ç½®")
    print("=" * 80)
    
    model_config = ModelConfig.from_name("qwen3-30b-a3b")
    
    # è®¾ç½®æ›´å¤§çš„é›†ç¾¤ (2 nodes Ã— 8 GPUs)
    hardware_config = HardwareConfig(
        gpu=GPUSpec.from_name("H100-80GB-HBM3"),
        num_nodes=2,
        gpus_per_node=8,
    )
    
    training_config = TrainingConfig(
        micro_batch_size=1,
        sequence_length=8192,
        gradient_accumulation_steps=128,
    )
    
    costmodel = PDCostModel(model_config, hardware_config, training_config)
    
    # ç”Ÿæˆæœç´¢ç©ºé—´
    total_gpus = hardware_config.total_gpus
    print(f"\næ€» GPU æ•°: {total_gpus}")
    
    configs = costmodel.generate_search_space(total_gpus, max_tp=8, max_pp=4)
    print(f"æœç´¢ç©ºé—´å¤§å°: {len(configs)}")
    
    # æœç´¢æœ€ä¼˜é…ç½®
    best_configs = costmodel.rank_configurations(configs, top_k=10)
    
    return best_configs


def example_dense_model():
    """ç¤ºä¾‹ 4: Dense æ¨¡å‹ (é MoE)"""
    print("\n" + "=" * 80)
    print("ğŸ“Œ ç¤ºä¾‹ 4: Dense æ¨¡å‹é¢„æµ‹ (LLaMA-3 70B)")
    print("=" * 80)
    
    model_config = ModelConfig.from_name("llama3-70b")
    print(f"\næ¨¡å‹: LLaMA-3 70B (Dense)")
    params = model_config.estimate_parameters()
    print(f"å‚æ•°é‡: {params['total_billion']:.2f}B")
    
    hardware_config = HardwareConfig(
        gpu=GPUSpec.from_name("H100-80GB-HBM3"),
        num_nodes=1,
        gpus_per_node=8,
    )
    
    training_config = TrainingConfig(
        micro_batch_size=1,
        sequence_length=4096,
        gradient_accumulation_steps=32,
    )
    
    costmodel = PDCostModel(model_config, hardware_config, training_config)
    
    # Dense æ¨¡å‹ä¸éœ€è¦ EP
    configs = [
        {"tp": 8, "pp": 1, "dp": 1, "ep": 1, "sharding": "stage1"},
        {"tp": 4, "pp": 2, "dp": 1, "ep": 1, "sharding": "stage1"},
        {"tp": 4, "pp": 1, "dp": 2, "ep": 1, "sharding": "stage2"},
        {"tp": 2, "pp": 4, "dp": 1, "ep": 1, "sharding": "stage1"},
    ]
    
    results = costmodel.rank_configurations(configs, top_k=4)
    
    return results


def example_custom_model():
    """ç¤ºä¾‹ 5: è‡ªå®šä¹‰æ¨¡å‹é…ç½®"""
    print("\n" + "=" * 80)
    print("ğŸ“Œ ç¤ºä¾‹ 5: è‡ªå®šä¹‰æ¨¡å‹é…ç½®")
    print("=" * 80)
    
    # åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹é…ç½®
    custom_model = ModelConfig(
        num_hidden_layers=32,
        hidden_size=4096,
        intermediate_size=11008,
        num_attention_heads=32,
        num_key_value_heads=8,
        head_dim=128,
        num_experts=64,
        num_experts_per_tok=4,
        moe_intermediate_size=2048,
        vocab_size=32000,
    )
    
    print(f"\nè‡ªå®šä¹‰æ¨¡å‹:")
    print(f"  - å±‚æ•°: {custom_model.num_hidden_layers}")
    print(f"  - hidden_size: {custom_model.hidden_size}")
    print(f"  - num_experts: {custom_model.num_experts}")
    print(f"  - topk: {custom_model.num_experts_per_tok}")
    
    params = custom_model.estimate_parameters()
    print(f"  - å‚æ•°é‡: {params['total_billion']:.2f}B")
    
    costmodel = PDCostModel(custom_model)
    
    parallel = ParallelConfig(tp=4, pp=1, dp=2, ep=4, sharding="stage1")
    result = costmodel.predict(parallel, micro_batch_size=2, seq_len=4096)
    
    print(f"\né¢„æµ‹ç»“æœ ({parallel}):")
    print(result)


def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("\n" + "ğŸš€" * 30)
    print("  pdcost - PaddleFormers åˆ†å¸ƒå¼è®­ç»ƒä»£ä»·æ¨¡å‹ç¤ºä¾‹")
    print("ğŸš€" * 30)
    
    # ç¤ºä¾‹ 1: å•é…ç½®é¢„æµ‹
    example_single_config()
    
    # ç¤ºä¾‹ 2: é…ç½®æ¯”è¾ƒ
    example_compare_configs()
    
    # ç¤ºä¾‹ 3: æœç´¢ç©ºé—´
    example_search_space()
    
    # ç¤ºä¾‹ 4: Dense æ¨¡å‹
    example_dense_model()
    
    # ç¤ºä¾‹ 5: è‡ªå®šä¹‰æ¨¡å‹
    example_custom_model()
    
    print("\n" + "âœ… æ‰€æœ‰ç¤ºä¾‹å®Œæˆ!")


if __name__ == "__main__":
    main()