#!/usr/bin/env python3
"""
PDCostModel - PaddleFormers åˆ†å¸ƒå¼è®­ç»ƒä»£ä»·æ¨¡å‹ä¸»æ¨¡å—

æ•´åˆ:
- ç¡¬ä»¶é…ç½®
- è®¡ç®—æ¨¡å‹
- é€šä¿¡æ¨¡å‹
- æ˜¾å­˜æ¨¡å‹

æä¾›ç»Ÿä¸€çš„é¢„æµ‹æ¥å£
"""

import json
import os
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple

from .config import (
    ModelConfig, ParallelConfig, TrainingConfig, HardwareConfig,
    GPUSpec, NetworkSpec, ShardingStage, RecomputeGranularity
)
from .memory_model import MemoryModel, MemoryBreakdown, ShardingConfig, RecomputeConfig
from .compute_model import ComputeModel
from .comm_model import CommModel


@dataclass
class PredictionResult:
    """é¢„æµ‹ç»“æœ"""
    # ========== æ—¶å»¶ (ms) ==========
    step_time_ms: float = 0.0  # æ€» step æ—¶é—´
    
    # è®¡ç®—æ—¶é—´
    compute_time_ms: float = 0.0
    forward_time_ms: float = 0.0
    backward_time_ms: float = 0.0
    
    # é€šä¿¡æ—¶é—´
    tp_comm_time_ms: float = 0.0
    dp_comm_time_ms: float = 0.0
    ep_comm_time_ms: float = 0.0
    pp_comm_time_ms: float = 0.0
    sp_comm_time_ms: float = 0.0
    total_comm_time_ms: float = 0.0
    
    # æµæ°´çº¿æ°”æ³¡
    bubble_time_ms: float = 0.0
    bubble_ratio: float = 0.0
    
    # ========== æ˜¾å­˜ (GB) ==========
    memory_gb: float = 0.0
    memory_breakdown: MemoryBreakdown = field(default_factory=MemoryBreakdown)
    fits_memory: bool = True
    
    # ========== æ•ˆç‡æŒ‡æ ‡ ==========
    compute_efficiency: float = 0.0  # è®¡ç®—æ•ˆç‡
    mfu: float = 0.0  # Model FLOPs Utilization
    
    # ========== ååé‡ ==========
    tokens_per_step: int = 0
    tokens_per_second: float = 0.0
    tokens_per_second_per_gpu: float = 0.0
    
    # ========== é…ç½®ä¿¡æ¯ ==========
    parallel_config: Dict = field(default_factory=dict)
    recompute_overhead: float = 1.0
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "time": {
                "step_time_ms": round(self.step_time_ms, 2),
                "compute_time_ms": round(self.compute_time_ms, 2),
                "forward_time_ms": round(self.forward_time_ms, 2),
                "backward_time_ms": round(self.backward_time_ms, 2),
                "total_comm_time_ms": round(self.total_comm_time_ms, 2),
                "tp_comm_time_ms": round(self.tp_comm_time_ms, 2),
                "dp_comm_time_ms": round(self.dp_comm_time_ms, 2),
                "ep_comm_time_ms": round(self.ep_comm_time_ms, 2),
                "pp_comm_time_ms": round(self.pp_comm_time_ms, 2),
                "bubble_time_ms": round(self.bubble_time_ms, 2),
                "bubble_ratio": round(self.bubble_ratio, 4),
            },
            "memory": self.memory_breakdown.to_dict(),
            "fits_memory": self.fits_memory,
            "efficiency": {
                "compute_efficiency": round(self.compute_efficiency, 4),
                "mfu": round(self.mfu, 4),
            },
            "throughput": {
                "tokens_per_step": self.tokens_per_step,
                "tokens_per_second": round(self.tokens_per_second, 0),
                "tokens_per_second_per_gpu": round(self.tokens_per_second_per_gpu, 0),
            },
            "config": self.parallel_config,
        }
    
    def __str__(self) -> str:
        fits_str = "âœ…" if self.fits_memory else "âŒ"
        return (
            f"PredictionResult:\n"
            f"  Step Time: {self.step_time_ms:.2f} ms\n"
            f"    - Compute: {self.compute_time_ms:.2f} ms\n"
            f"    - Communication: {self.total_comm_time_ms:.2f} ms\n"
            f"    - Bubble: {self.bubble_time_ms:.2f} ms ({self.bubble_ratio:.1%})\n"
            f"  Memory: {self.memory_gb:.2f} GB {fits_str}\n"
            f"  MFU: {self.mfu:.1%}\n"
            f"  Throughput: {self.tokens_per_second:,.0f} tok/s "
            f"({self.tokens_per_second_per_gpu:,.0f} tok/s/GPU)"
        )


class PDCostModel:
    """
    PaddleFormers åˆ†å¸ƒå¼è®­ç»ƒä»£ä»·æ¨¡å‹
    
    ç”¨äºé¢„æµ‹ä¸åŒå¹¶è¡Œé…ç½®ä¸‹çš„:
    - Step æ—¶é—´
    - æ˜¾å­˜å ç”¨
    - ç¡¬ä»¶åˆ©ç”¨ç‡
    - è®­ç»ƒååé‡
    
    ä½¿ç”¨ç¤ºä¾‹:
        model_config = ModelConfig.from_name("qwen3-30b-a3b")
        costmodel = PDCostModel(model_config)
        
        parallel = ParallelConfig(tp=8, pp=1, dp=1, ep=8)
        result = costmodel.predict(parallel, micro_batch_size=1, seq_len=8192)
        print(result)
        
    å¸¦ç¡¬ä»¶æ ¡å‡†:
        costmodel = PDCostModel(model_config, auto_calibrate=True)
        # æˆ–æ‰‹åŠ¨æ ¡å‡†
        costmodel.calibrate()
    
    æ¡†æ¶ç‰¹æ€§è¯´æ˜:
        PaddleFormers æ¡†æ¶æœ‰ä¸€ä¸ªé‡è¦ç‰¹æ€§ï¼šå­˜åœ¨"æœ€å°è®¡ç®—æ‰¹æ¬¡"
        - å½“ seq_len <= min_compute_seq_len æ—¶ï¼Œstep time åŸºæœ¬æ’å®š
        - å½“ seq_len > min_compute_seq_len æ—¶ï¼Œstep time çº¿æ€§å¢é•¿
        è¿™æ˜¯å› ä¸ºæ¡†æ¶ä¼šå°†å°åºåˆ—å¡«å……åˆ°æœ€å°è®¡ç®—å•å…ƒå¤§å°
    """
    
    # ========== æ¡†æ¶æ ¡å‡†å‚æ•°ï¼ˆä»å®éªŒæ•°æ®æ‹Ÿåˆï¼‰==========
    # è¿™äº›å‚æ•°æ˜¯é’ˆå¯¹ PaddleFormers + Qwen3-30B-A3B çš„ç»éªŒå€¼
    # ä¸åŒæ¨¡å‹/æ¡†æ¶å¯èƒ½éœ€è¦é‡æ–°æ ¡å‡†
    
    # æœ€å°è®¡ç®—åºåˆ—é•¿åº¦é˜ˆå€¼
    MIN_COMPUTE_SEQ_LEN = 2048
    
    # åŸºç¡€ step time (seq_len <= MIN_COMPUTE_SEQ_LEN æ—¶çš„å›ºå®šæ—¶é—´)
    # æ³¨æ„ï¼šè¿™ä¸ªå€¼ä¼šæ ¹æ® offload é…ç½®åŠ¨æ€è°ƒæ•´
    BASE_STEP_TIME_S = 12.75
    
    # åºåˆ—é•¿åº¦å¢é•¿æ–œç‡ (seq_len > MIN_COMPUTE_SEQ_LEN æ—¶)
    SEQ_LEN_SLOPE_MS_PER_TOKEN = 2.6  # ms/token
    
    # Offload æ—¶é—´ä¼°ç®—å‚æ•°
    PCIE_BANDWIDTH_GBPS = 16.0  # PCIe 4.0 x16
    OFFLOAD_OVERLAP_RATIO = 0.0  # å®æµ‹ offload åŸºæœ¬æ— æ³• overlap
    
    def __init__(self, 
                 model_config: ModelConfig,
                 hardware_config: HardwareConfig = None,
                 training_config: TrainingConfig = None,
                 auto_calibrate: bool = False,
                 calibrate_on_predict: bool = False):
        """
        åˆå§‹åŒ– CostModel
        
        Args:
            model_config: æ¨¡å‹æ¶æ„é…ç½®
            hardware_config: ç¡¬ä»¶é…ç½® (é»˜è®¤ H100-80GBï¼Œå¦‚æœ auto_calibrate=True åˆ™è‡ªåŠ¨æ£€æµ‹)
            training_config: è®­ç»ƒé…ç½® (é»˜è®¤ bf16, recompute=full)
            auto_calibrate: æ˜¯å¦åœ¨åˆå§‹åŒ–æ—¶è‡ªåŠ¨æ ¡å‡†ç¡¬ä»¶
            calibrate_on_predict: æ˜¯å¦åœ¨æ¯æ¬¡é¢„æµ‹å‰é‡æ–°æ ¡å‡†
        """
        self.model_config = model_config
        self.training_config = training_config or TrainingConfig()
        self._calibrated = False
        self._calibrate_on_predict = calibrate_on_predict
        self._calibration_result = None
        
        # ç¡¬ä»¶é…ç½®
        if auto_calibrate:
            self.hardware_config = self.calibrate(verbose=True)
        else:
            self.hardware_config = hardware_config or HardwareConfig()
        
        # åˆå§‹åŒ–å­æ¨¡å‹
        self._init_sub_models()
    
    def _init_sub_models(self):
        """åˆå§‹åŒ–å­æ¨¡å‹"""
        self.memory_model = MemoryModel(self.model_config, self.training_config)
        self.compute_model = ComputeModel(self.model_config, self.hardware_config, self.training_config)
        self.comm_model = CommModel(self.hardware_config)
    
    def calibrate(self, 
                  num_nodes: int = 1,
                  gpus_per_node: int = None,
                  device_id: int = 0,
                  test_compute: bool = True,
                  test_memory: bool = True,
                  gemm_size: int = 8192,
                  verbose: bool = True) -> HardwareConfig:
        """
        æ‰§è¡Œç¡¬ä»¶æ ¡å‡†
        
        é€šè¿‡å®é™…è¿è¡Œ benchmark æµ‹è¯• GPU ç®—åŠ›å’Œæ˜¾å­˜å¸¦å®½ï¼Œ
        ç„¶åæ›´æ–° HardwareConfig
        
        Args:
            num_nodes: èŠ‚ç‚¹æ•°
            gpus_per_node: æ¯èŠ‚ç‚¹ GPU æ•° (é»˜è®¤è‡ªåŠ¨æ£€æµ‹)
            device_id: æµ‹è¯•ä½¿ç”¨çš„ GPU ID
            test_compute: æ˜¯å¦æµ‹è¯•ç®—åŠ›
            test_memory: æ˜¯å¦æµ‹è¯•æ˜¾å­˜å¸¦å®½
            gemm_size: GEMM æµ‹è¯•çŸ©é˜µå¤§å°
            verbose: æ˜¯å¦æ‰“å°è¿›åº¦
        
        Returns:
            HardwareConfig: æ ¡å‡†åçš„ç¡¬ä»¶é…ç½®
        """
        from .calibration import HardwareCalibrator
        
        calibrator = HardwareCalibrator(device_id=device_id)
        self._calibration_result = calibrator.calibrate(
            test_compute=test_compute,
            test_memory=test_memory,
            gemm_size=gemm_size,
            verbose=verbose
        )
        
        self.hardware_config = calibrator.create_hardware_config(
            num_nodes=num_nodes,
            gpus_per_node=gpus_per_node
        )
        
        # é‡æ–°åˆå§‹åŒ–å­æ¨¡å‹
        self._init_sub_models()
        self._calibrated = True
        
        return self.hardware_config
    
    @property
    def calibration_result(self):
        """è·å–æ ¡å‡†ç»“æœ"""
        return self._calibration_result
    
    @property
    def is_calibrated(self) -> bool:
        """æ˜¯å¦å·²æ ¡å‡†"""
        return self._calibrated
    
    def predict(self,
                parallel: ParallelConfig,
                micro_batch_size: int = None,
                seq_len: int = None,
                max_seq_len: int = None,
                gradient_accumulation_steps: int = None,
                recompute_granularity: str = None,
                tensorwise_offload_optimizer: bool = None,
                tensorwise_offload_ratio: float = None,
                split_param: bool = True,
                sd_release_grads: bool = False) -> PredictionResult:
        """
        é¢„æµ‹ç»™å®šå¹¶è¡Œé…ç½®çš„æ€§èƒ½
        
        Args:
            parallel: å¹¶è¡Œé…ç½®
            micro_batch_size: micro batch size (é»˜è®¤ä½¿ç”¨ training_config)
            seq_len: åºåˆ—é•¿åº¦ (é»˜è®¤ä½¿ç”¨ training_config.sequence_length)
            max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦ï¼Œç”¨äºæ¿€æ´»æ˜¾å­˜ä¼°ç®— (é»˜è®¤ç­‰äº seq_len)
            gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
            recompute_granularity: é‡è®¡ç®—ç²’åº¦ ("none", "selective", "full")
            tensorwise_offload_optimizer: æ˜¯å¦å¯ç”¨ tensorwise ä¼˜åŒ–å™¨ offload
            tensorwise_offload_ratio: tensorwise offload æ¯”ä¾‹ (é»˜è®¤ 0.95)
            split_param: PaddleFormers ShardingV2 å‚æ•°åˆ†ç‰‡ (é»˜è®¤ True)
            sd_release_grads: è¿­ä»£åé‡Šæ”¾æ¢¯åº¦ï¼Œé™ä½å³°å€¼æ˜¾å­˜ (é»˜è®¤ False)
        
        Returns:
            PredictionResult: é¢„æµ‹ç»“æœ
        """
        # ä½¿ç”¨é»˜è®¤å€¼
        if micro_batch_size is None:
            micro_batch_size = self.training_config.micro_batch_size
        if seq_len is None:
            seq_len = self.training_config.sequence_length
        if max_seq_len is None:
            max_seq_len = seq_len  # é»˜è®¤ä½¿ç”¨å½“å‰ seq_len
        if gradient_accumulation_steps is None:
            gradient_accumulation_steps = self.training_config.gradient_accumulation_steps
        
        # é‡è®¡ç®—é…ç½®
        if recompute_granularity is None:
            recompute_gran = self.training_config.recompute_config
        else:
            recompute_gran_map = {
                "none": RecomputeGranularity.NONE,
                "selective": RecomputeGranularity.SELECTIVE,
                "full": RecomputeGranularity.FULL,
            }
            recompute_gran = recompute_gran_map.get(recompute_granularity.lower(), RecomputeGranularity.FULL)
        
        # tensorwise offload é…ç½®
        use_tensorwise = tensorwise_offload_optimizer if tensorwise_offload_optimizer is not None else False
        offload_ratio = tensorwise_offload_ratio if tensorwise_offload_ratio is not None else 0.95
        
        result = PredictionResult()
        result.parallel_config = parallel.to_dict()
        
        # ========== æ˜¾å­˜é¢„æµ‹ ==========
        sharding_config = ShardingConfig(
            stage=parallel.sharding_stage,
            degree=parallel.effective_sharding_degree,
            split_param=split_param,
            release_grads=sd_release_grads,
            tensorwise_offload=use_tensorwise,
            tensorwise_offload_ratio=offload_ratio,
        )
        
        recompute_config = RecomputeConfig(
            granularity=recompute_gran,
            method=self.training_config.recompute_method,
            num_layers=self.training_config.recompute_num_layers,
        )
        
        result.memory_breakdown = self.memory_model.estimate_memory(
            parallel, sharding_config, recompute_config, max_seq_len
        )
        result.memory_gb = result.memory_breakdown.total_memory_gb
        result.fits_memory = result.memory_gb <= self.hardware_config.gpu.memory_gb
        
        # ========== è®¡ç®—æ—¶é—´é¢„æµ‹ ==========
        result.recompute_overhead = recompute_config.get_recompute_overhead()
        
        compute_result = self.compute_model.estimate_step_compute_time(
            micro_batch_size, seq_len, parallel,
            gradient_accumulation_steps, result.recompute_overhead
        )
        
        result.forward_time_ms = compute_result["forward_time_ms"]
        result.backward_time_ms = compute_result["backward_time_ms"]
        result.bubble_time_ms = compute_result["bubble_time_ms"]
        result.compute_time_ms = compute_result["compute_time_ms"]
        result.bubble_ratio = compute_result["bubble_ratio"]
        
        # ========== é€šä¿¡æ—¶é—´é¢„æµ‹ ==========
        comm_result = self.comm_model.estimate_step_comm_time(
            self.model_config, self.training_config,
            parallel, gradient_accumulation_steps
        )
        
        result.tp_comm_time_ms = comm_result["tp_comm_time_ms"]
        result.dp_comm_time_ms = comm_result["dp_comm_time_ms"]
        result.ep_comm_time_ms = comm_result["ep_comm_time_ms"]
        result.pp_comm_time_ms = comm_result["pp_comm_time_ms"]
        result.sp_comm_time_ms = comm_result.get("sp_comm_time_ms", 0)
        result.total_comm_time_ms = comm_result["total_comm_time_ms"]
        
        # ========== é¢å¤–å¼€é”€ ==========
        # 1. tensorwise_offload çš„ CPU-GPU æ•°æ®ä¼ è¾“å¼€é”€
        offload_overhead_ms = 0.0
        if use_tensorwise:
            # ä¼°ç®—ä¼˜åŒ–å™¨æ›´æ–°æ—¶çš„ CPU-GPU ä¼ è¾“æ—¶é—´
            # tensorwise_offload éœ€è¦ï¼š
            # - ä» CPU åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€åˆ° GPU
            # - åœ¨ GPU ä¸Šæ‰§è¡Œæ›´æ–°
            # - å°†æ›´æ–°åçš„çŠ¶æ€å†™å› CPU
            # 
            # å…³é”®ç‚¹ï¼š
            # 1. æ¯ä¸ª GPU åª offload è‡ªå·±è´Ÿè´£çš„å‚æ•°ï¼ˆSharding åˆ‡åˆ†åï¼‰
            # 2. tensorwise æ˜¯æµå¼ä¼ è¾“ï¼Œå¯ä»¥å’Œè®¡ç®—éƒ¨åˆ† overlap
            # 3. å®é™… overlap æ•ˆç‡çº¦ 50%
            
            param_count = self.model_config.estimate_parameters()["total"]
            # Sharding åˆ‡åˆ†åæ¯ä¸ª GPU çš„å‚æ•°é‡
            sharding_degree = parallel.effective_sharding_degree
            params_per_gpu = param_count / sharding_degree
            
            # AdamW: 2 ä¸ª fp32 çŠ¶æ€ + master weight = 12 bytes per param
            offload_bytes = params_per_gpu * 12 * offload_ratio
            
            # CPU-GPU å¸¦å®½çº¦ 16 GB/s (PCIe 4.0 x16)
            # åŒå‘ä¼ è¾“ï¼Œä½†å¯ä»¥å’Œè®¡ç®— overlap çº¦ 50%
            cpu_gpu_bandwidth_gbps = 16.0
            offload_time_raw = offload_bytes * 2 / (cpu_gpu_bandwidth_gbps * 1e9) * 1000
            offload_overhead_ms = offload_time_raw * 0.5  # 50% å¯ä»¥ overlap
        
        # 2. å° batch size æ•ˆç‡æƒ©ç½š
        # batch_size=1 æ—¶ GPU åˆ©ç”¨ç‡æä½ï¼Œkernel launch å¼€é”€å æ¯”å¤§
        batch_efficiency = min(1.0, micro_batch_size / 4.0) * 0.5 + 0.5
        
        # 3. æ¡†æ¶å¼€é”€ (åŠ¨æ€å›¾ã€Python è°ƒåº¦ç­‰)
        # çº¦ 10-20% çš„é¢å¤–å¼€é”€
        framework_overhead_factor = 1.15
        
        # 4. MoE è´Ÿè½½ä¸å‡è¡¡å¼€é”€
        moe_lb_overhead = 1.0
        if parallel.ep > 1 and self.model_config.num_moe_layers > 0:
            # è´Ÿè½½ä¸å‡è¡¡å¯¼è‡´çº¦ 10-20% çš„é¢å¤–ç­‰å¾…æ—¶é—´
            moe_lb_overhead = 1.15
        
        # ========== æ€»æ—¶å»¶ ==========
        # é€šä¿¡ä¸è®¡ç®—çš„é‡å 
        # TP é€šä¿¡åœ¨å…³é”®è·¯å¾„ä¸Š
        # DP é€šä¿¡å¯ä»¥ä¸åå‘éƒ¨åˆ† overlap
        # EP é€šä¿¡åœ¨ MoE å±‚å…³é”®è·¯å¾„ä¸Š
        
        overlap_factor = 0.3  # å‡è®¾ 30% çš„é€šä¿¡å¯ä»¥ overlap
        effective_comm_time = (
            result.tp_comm_time_ms +
            result.ep_comm_time_ms +
            result.pp_comm_time_ms +
            result.dp_comm_time_ms * (1 - overlap_factor) +
            result.sp_comm_time_ms
        )
        
        # è®¡ç®—æ—¶é—´åŠ ä¸Šå„ç§å¼€é”€
        adjusted_compute_time = result.compute_time_ms / batch_efficiency * framework_overhead_factor * moe_lb_overhead
        
        result.step_time_ms = adjusted_compute_time + effective_comm_time + offload_overhead_ms
        
        # ========== æ•ˆç‡æŒ‡æ ‡ ==========
        result.compute_efficiency = self._calculate_compute_efficiency(result, parallel)
        result.mfu = self._calculate_mfu(
            result, parallel, micro_batch_size, seq_len, gradient_accumulation_steps
        )
        
        # ========== ååé‡ ==========
        result.tokens_per_step, result.tokens_per_second, result.tokens_per_second_per_gpu = \
            self._calculate_throughput(result, parallel, micro_batch_size, seq_len, gradient_accumulation_steps)
        
        return result
    
    def predict_calibrated(self,
                           parallel: ParallelConfig,
                           micro_batch_size: int = None,
                           seq_len: int = None,
                           max_seq_len: int = None,
                           gradient_accumulation_steps: int = None,
                           recompute_granularity: str = None,
                           tensorwise_offload_optimizer: bool = None,
                           tensorwise_offload_ratio: float = None,
                           split_param: bool = True,
                           sd_release_grads: bool = False) -> PredictionResult:
        """
        ä½¿ç”¨æ ¡å‡†åçš„åˆ†æ®µçº¿æ€§æ¨¡å‹è¿›è¡Œé¢„æµ‹
        
        è¿™ä¸ªæ–¹æ³•ä½¿ç”¨ä»å®éªŒæ•°æ®æ‹Ÿåˆçš„æ¨¡å‹ï¼Œæ›´å‡†ç¡®åœ°é¢„æµ‹ step timeã€‚
        
        æ¨¡å‹å…¬å¼:
            if seq_len <= MIN_COMPUTE_SEQ_LEN:
                step_time = BASE_STEP_TIME_S
            else:
                step_time = BASE_STEP_TIME_S + SEQ_LEN_SLOPE * (seq_len - MIN_COMPUTE_SEQ_LEN)
        
        å…³é”®å‘ç°ï¼š
            1. PaddleFormers æ¡†æ¶å­˜åœ¨"æœ€å°è®¡ç®—æ‰¹æ¬¡"(~2048 tokens)
            2. seq_len <= 2048 æ—¶ï¼Œstep time åŸºæœ¬æ’å®šï¼ˆå— offload ä¸»å¯¼ï¼‰
            3. seq_len > 2048 æ—¶ï¼Œstep time çº¿æ€§å¢é•¿
        
        Args:
            parallel: å¹¶è¡Œé…ç½®
            å…¶ä»–å‚æ•°ä¸ predict() ç›¸åŒ
        
        Returns:
            PredictionResult: é¢„æµ‹ç»“æœ
        """
        # ä½¿ç”¨é»˜è®¤å€¼
        if micro_batch_size is None:
            micro_batch_size = self.training_config.micro_batch_size
        if seq_len is None:
            seq_len = self.training_config.sequence_length
        if max_seq_len is None:
            max_seq_len = seq_len
        if gradient_accumulation_steps is None:
            gradient_accumulation_steps = self.training_config.gradient_accumulation_steps
        
        # tensorwise offload é…ç½®
        use_tensorwise = tensorwise_offload_optimizer if tensorwise_offload_optimizer is not None else False
        offload_ratio = tensorwise_offload_ratio if tensorwise_offload_ratio is not None else 0.95
        
        # é‡è®¡ç®—é…ç½®
        if recompute_granularity is None:
            recompute_gran = self.training_config.recompute_config
        else:
            recompute_gran_map = {
                "none": RecomputeGranularity.NONE,
                "selective": RecomputeGranularity.SELECTIVE,
                "full": RecomputeGranularity.FULL,
            }
            recompute_gran = recompute_gran_map.get(recompute_granularity.lower(), RecomputeGranularity.FULL)
        
        result = PredictionResult()
        result.parallel_config = parallel.to_dict()
        
        # ========== æ˜¾å­˜é¢„æµ‹ï¼ˆä½¿ç”¨åŸæœ‰é€»è¾‘ï¼‰==========
        sharding_config = ShardingConfig(
            stage=parallel.sharding_stage,
            degree=parallel.effective_sharding_degree,
            split_param=split_param,
            release_grads=sd_release_grads,
            tensorwise_offload=use_tensorwise,
            tensorwise_offload_ratio=offload_ratio,
        )
        
        recompute_config = RecomputeConfig(
            granularity=recompute_gran,
            method=self.training_config.recompute_method,
            num_layers=self.training_config.recompute_num_layers,
        )
        
        result.memory_breakdown = self.memory_model.estimate_memory(
            parallel, sharding_config, recompute_config, max_seq_len
        )
        result.memory_gb = result.memory_breakdown.total_memory_gb
        result.fits_memory = result.memory_gb <= self.hardware_config.gpu.memory_gb
        result.recompute_overhead = recompute_config.get_recompute_overhead()
        
        # ========== Step Time é¢„æµ‹ï¼ˆä½¿ç”¨åˆ†æ®µçº¿æ€§æ¨¡å‹ï¼‰==========
        # 1. è®¡ç®— Offload æ—¶é—´ï¼ˆå›ºå®šå¼€é”€ï¼‰
        offload_time_ms = 0.0
        if use_tensorwise:
            param_count = self.model_config.estimate_parameters()["total"]
            sharding_degree = parallel.effective_sharding_degree
            params_per_gpu = param_count / sharding_degree
            offload_bytes = params_per_gpu * 12 * offload_ratio  # AdamW states
            # åŒå‘ä¼ è¾“ï¼Œå‡ ä¹æ—  overlap
            offload_time_ms = offload_bytes * 2 / (self.PCIE_BANDWIDTH_GBPS * 1e9) * 1000 * (1 - self.OFFLOAD_OVERLAP_RATIO)
        
        # 2. ä½¿ç”¨åˆ†æ®µçº¿æ€§æ¨¡å‹è®¡ç®— step time
        # åŸºç¡€å‚æ•°æ˜¯åœ¨ä»¥ä¸‹æ¡ä»¶ä¸‹æµ‹å¾—çš„ï¼š
        # - seq_len=2048, mbs=1, gas=16, offload=0.95
        # - å¯¹åº” step_time = 12.75s
        
        # åˆ†æ®µçº¿æ€§æ¨¡å‹ï¼ˆç›´æ¥é¢„æµ‹æ€» step timeï¼Œå·²åŒ…å« offloadï¼‰
        if seq_len <= self.MIN_COMPUTE_SEQ_LEN:
            # seq_len <= é˜ˆå€¼æ—¶ï¼Œstep time å›ºå®š
            base_step_time_ms = self.BASE_STEP_TIME_S * 1000
        else:
            # seq_len > é˜ˆå€¼æ—¶ï¼Œstep time çº¿æ€§å¢é•¿
            extra_time_ms = self.SEQ_LEN_SLOPE_MS_PER_TOKEN * (seq_len - self.MIN_COMPUTE_SEQ_LEN)
            base_step_time_ms = self.BASE_STEP_TIME_S * 1000 + extra_time_ms
        
        # 3. æ ¹æ®å…¶ä»–é…ç½®è°ƒæ•´
        # åŸºå‡†é…ç½®ï¼šmbs=1, gas=16
        
        # 3.1 micro_batch_size ç¼©æ”¾
        # mbs=1 æ˜¯åŸºå‡†ï¼Œå¢åŠ  mbs ä¼šå¢åŠ è®¡ç®—é‡ä½†æ•ˆç‡ä¹Ÿä¼šæå‡
        if micro_batch_size == 1:
            mbs_factor = 1.0
        else:
            # mbs > 1 æ—¶ï¼Œè®¡ç®—æ—¶é—´å¢åŠ ä½†æ•ˆç‡ä¹Ÿæå‡
            # å‡è®¾æ•ˆç‡æå‡ 15%ï¼Œå³æ¯å¢åŠ  1 ä¸ª mbsï¼Œè®¡ç®—é‡å¢åŠ  0.85 å€
            mbs_factor = 1.0 + (micro_batch_size - 1) * 0.85
        
        # 3.2 gradient_accumulation_steps ç¼©æ”¾ï¼ˆåŸºå‡†æ˜¯ gas=16ï¼‰
        gas_factor = gradient_accumulation_steps / 16.0
        
        # è°ƒæ•´ step time
        # æ³¨æ„ï¼šoffload æ—¶é—´ä¸éš mbs/gas ç¼©æ”¾
        # åˆ†ç¦» offload å’Œè®¡ç®—æ—¶é—´è¿›è¡Œç¼©æ”¾
        offload_portion_ms = 6800  # ä¼°è®¡ offload çº¦ 6.8s
        compute_portion_ms = base_step_time_ms - offload_portion_ms
        
        # åªæœ‰è®¡ç®—éƒ¨åˆ†éœ€è¦ç¼©æ”¾
        scaled_compute_ms = compute_portion_ms * mbs_factor * gas_factor
        
        # 4. æ€» step time
        step_time_ms = scaled_compute_ms + offload_portion_ms
        
        # å¦‚æœä¸ä½¿ç”¨ offloadï¼Œéœ€è¦è°ƒæ•´
        if not use_tensorwise:
            # æ—  offload æ—¶ï¼Œåªæœ‰è®¡ç®—éƒ¨åˆ†
            step_time_ms = scaled_compute_ms
        result.step_time_ms = step_time_ms
        
        # ========== åˆ†è§£æ—¶é—´ï¼ˆç”¨äºæŠ¥å‘Šï¼‰==========
        # ä¼°ç®—å„éƒ¨åˆ†æ—¶é—´å æ¯”
        actual_compute_ms = scaled_compute_ms if use_tensorwise else step_time_ms
        result.compute_time_ms = actual_compute_ms * 0.85  # è®¡ç®—å  85%
        result.total_comm_time_ms = actual_compute_ms * 0.15  # é€šä¿¡å  15%
        result.forward_time_ms = result.compute_time_ms * 0.33
        result.backward_time_ms = result.compute_time_ms * 0.67
        result.bubble_time_ms = 0.0
        result.bubble_ratio = 0.0
        
        if parallel.pp > 1:
            bubble_ratio = (parallel.pp - 1) / gradient_accumulation_steps
            result.bubble_time_ms = step_time_ms * bubble_ratio
            result.bubble_ratio = bubble_ratio
        
        # ========== æ•ˆç‡å’Œååé‡ ==========
        result.compute_efficiency = self._calculate_compute_efficiency(result, parallel)
        result.mfu = self._calculate_mfu(
            result, parallel, micro_batch_size, seq_len, gradient_accumulation_steps
        )
        result.tokens_per_step, result.tokens_per_second, result.tokens_per_second_per_gpu = \
            self._calculate_throughput(result, parallel, micro_batch_size, seq_len, gradient_accumulation_steps)
        
        return result
    
    def _calculate_compute_efficiency(self, result: PredictionResult,
                                      parallel: ParallelConfig) -> float:
        """è®¡ç®—æ•ˆç‡"""
        if result.step_time_ms <= 0:
            return 0.0
        
        # è®¡ç®—æ—¶é—´å æ¯”
        compute_ratio = result.compute_time_ms / result.step_time_ms
        
        # å¹¶è¡Œæ•ˆç‡æŸå¤±
        tp_efficiency = 0.9 if parallel.tp > 1 else 1.0
        pp_efficiency = 1.0 - result.bubble_ratio
        ep_efficiency = 0.85 if parallel.ep > 1 else 1.0
        
        return compute_ratio * tp_efficiency * pp_efficiency * ep_efficiency
    
    def _calculate_mfu(self, result: PredictionResult,
                       parallel: ParallelConfig,
                       micro_batch_size: int,
                       seq_len: int,
                       gradient_accumulation_steps: int) -> float:
        """
        è®¡ç®— Model FLOPs Utilization (MFU)
        
        MFU = å®é™…è®¡ç®—çš„ FLOPs / (å³°å€¼ç®—åŠ› Ã— æ—¶é—´)
        
        å¯¹äº MoE æ¨¡å‹ï¼Œè€ƒè™‘ç¨€ç–æ¿€æ´» (åªæœ‰ TopK ä¸ªä¸“å®¶å‚ä¸è®¡ç®—)
        """
        if result.step_time_ms <= 0:
            return 0.0
        
        h = self.model_config.hidden_size
        num_layers = self.model_config.num_hidden_layers
        kv_heads = self.model_config.num_key_value_heads
        head_dim = self.model_config.head_dim
        
        # ========== Attention FLOPs (æ¯ token) ==========
        # Q proj: h * h, K proj: h * kv_size, V proj: h * kv_size, O proj: h * h
        # QK^T + softmax + score*V: çº¦ 4 * h * seq_len (ç®€åŒ–)
        kv_size = kv_heads * head_dim
        attention_flops = 2 * (2 * h * h + 2 * h * kv_size) + 4 * h * seq_len
        
        # ========== FFN FLOPs (æ¯ token) ==========
        if self.model_config.num_experts > 1:
            # MoE: åªæœ‰ TopK ä¸ªä¸“å®¶æ¿€æ´»
            moe_ffn = self.model_config.moe_intermediate_size
            topk = self.model_config.num_experts_per_tok
            # Gate + Up + Down for TopK experts
            ffn_flops = 2 * 3 * h * moe_ffn * topk
            # Router
            router_flops = 2 * h * self.model_config.num_experts
            moe_layer_flops = attention_flops + ffn_flops + router_flops
            
            # Dense layers
            dense_ffn = self.model_config.intermediate_size
            dense_layer_flops = attention_flops + 2 * 3 * h * dense_ffn
            
            # æ··åˆ
            flops_per_token = (
                dense_layer_flops * self.model_config.num_dense_layers +
                moe_layer_flops * self.model_config.num_moe_layers
            )
        else:
            # Dense model
            ffn = self.model_config.intermediate_size
            layer_flops = attention_flops + 2 * 3 * h * ffn
            flops_per_token = layer_flops * num_layers
        
        # æ€» tokens
        tokens = micro_batch_size * seq_len
        total_tokens = tokens * gradient_accumulation_steps * parallel.dp
        
        # æ€» FLOPs (å‰å‘ + åå‘ â‰ˆ 3x å‰å‘)
        total_flops = flops_per_token * total_tokens * 3
        
        # å³°å€¼ FLOPs (æ‰€æœ‰ GPU)
        world_size = parallel.dp * parallel.tp * parallel.pp
        peak_tflops = self.hardware_config.gpu.get_tflops(self.training_config.dtype)
        peak_flops = peak_tflops * 1e12 * world_size * (result.step_time_ms / 1000)
        
        # MFU = å®é™…è®¡ç®—é‡ / ç†è®ºæœ€å¤§è®¡ç®—é‡
        mfu = total_flops / peak_flops if peak_flops > 0 else 0.0
        
        return min(mfu, 1.0)
    
    def _calculate_throughput(self, result: PredictionResult,
                              parallel: ParallelConfig,
                              micro_batch_size: int,
                              seq_len: int,
                              gradient_accumulation_steps: int) -> Tuple[int, float, float]:
        """è®¡ç®—ååé‡"""
        if result.step_time_ms <= 0:
            return 0, 0.0, 0.0
        
        # æ¯ step tokens æ•°
        tokens_per_step = micro_batch_size * seq_len * gradient_accumulation_steps * parallel.dp
        
        # æ€»ååé‡
        step_time_seconds = result.step_time_ms / 1000.0
        tokens_per_second = tokens_per_step / step_time_seconds
        
        # æ¯å¡ååé‡
        world_size = parallel.dp * parallel.tp * parallel.pp
        tokens_per_second_per_gpu = tokens_per_second / world_size
        
        return tokens_per_step, tokens_per_second, tokens_per_second_per_gpu
    
    def rank_configurations(self, configs: List[Dict], 
                            top_k: int = 10,
                            micro_batch_size: int = None,
                            seq_len: int = None) -> List[Dict]:
        """
        å¯¹å¹¶è¡Œé…ç½®åˆ—è¡¨è¿›è¡Œæ’åº
        
        æ’åºä¾æ®:
        1. æ˜¯å¦æ»¡è¶³æ˜¾å­˜çº¦æŸ
        2. step æ—¶å»¶ (è¶Šå°è¶Šå¥½)
        3. ç¡¬ä»¶åˆ©ç”¨ç‡
        
        Args:
            configs: å¹¶è¡Œé…ç½®åˆ—è¡¨
            top_k: è¿”å›å‰ k ä¸ªæœ€ä¼˜é…ç½®
            micro_batch_size: micro batch size
            seq_len: åºåˆ—é•¿åº¦
        
        Returns:
            æ’åºåçš„é…ç½®åˆ—è¡¨
        """
        results = []
        
        for cfg in configs:
            try:
                parallel = ParallelConfig.from_dict(cfg)
                prediction = self.predict(parallel, micro_batch_size, seq_len)
                
                results.append({
                    "rank": 0,
                    "config": cfg,
                    "config_str": str(parallel),
                    "step_time_ms": prediction.step_time_ms,
                    "memory_gb": prediction.memory_gb,
                    "fits_memory": prediction.fits_memory,
                    "mfu": prediction.mfu,
                    "tokens_per_second": prediction.tokens_per_second,
                    "tokens_per_second_per_gpu": prediction.tokens_per_second_per_gpu,
                    "prediction": prediction.to_dict(),
                })
            except Exception as e:
                print(f"Warning: Failed to predict config {cfg}: {e}")
                continue
        
        # æ’åº: å…ˆæŒ‰æ˜¯å¦æ»¡è¶³æ˜¾å­˜ï¼Œå†æŒ‰æ—¶å»¶
        results.sort(key=lambda x: (not x["fits_memory"], x["step_time_ms"]))
        
        # æ›´æ–°æ’å
        for i, r in enumerate(results):
            r["rank"] = i + 1
        
        # æ‰“å°æŠ¥å‘Š
        self._print_ranking_report(results[:top_k])
        
        return results[:top_k]
    
    def _print_ranking_report(self, results: List[Dict]):
        """æ‰“å°æ’åºæŠ¥å‘Š"""
        if not results:
            return
        
        print("\n" + "=" * 120)
        print("ğŸš€ PDCostModel - å¹¶è¡Œé…ç½®æ’åºæŠ¥å‘Š")
        print("=" * 120)
        print(f"{'æ’å':<4} {'é…ç½®':<30} {'æ—¶å»¶(ms)':<12} {'æ˜¾å­˜(GB)':<10} "
              f"{'çº¦æŸ':<6} {'MFU':<8} {'tok/s':<12} {'tok/s/GPU':<12}")
        print("-" * 120)
        
        for r in results:
            fits = "âœ…" if r["fits_memory"] else "âŒ"
            print(f"{r['rank']:<4} {r['config_str']:<30} "
                  f"{r['step_time_ms']:<12.2f} {r['memory_gb']:<10.2f} "
                  f"{fits:<6} {r['mfu']:<8.1%} "
                  f"{r['tokens_per_second']:<12,.0f} "
                  f"{r['tokens_per_second_per_gpu']:<12,.0f}")
        
        print("-" * 120)
        
        if results:
            best = results[0]
            print(f"\nğŸ“Š æœ€ä¼˜é…ç½®: {best['config_str']}")
            print(f"   â€¢ é¢„è®¡æ—¶å»¶: {best['step_time_ms']:.2f} ms")
            print(f"   â€¢ æ˜¾å­˜å ç”¨: {best['memory_gb']:.2f} GB")
            print(f"   â€¢ MFU: {best['mfu']:.1%}")
            print(f"   â€¢ ååé‡: {best['tokens_per_second']:,.0f} tok/s")
    
    def generate_search_space(self, total_gpus: int,
                              max_tp: int = 8,
                              max_pp: int = 8) -> List[Dict]:
        """
        ç”Ÿæˆå¹¶è¡Œé…ç½®æœç´¢ç©ºé—´
        
        Args:
            total_gpus: æ€» GPU æ•°
            max_tp: æœ€å¤§ TP åº¦
            max_pp: æœ€å¤§ PP åº¦
        
        Returns:
            é…ç½®åˆ—è¡¨
        """
        configs = []
        num_experts = self.model_config.num_experts
        
        for tp in [1, 2, 4, 8]:
            if tp > max_tp or tp > total_gpus:
                continue
            
            for pp in [1, 2, 4, 8]:
                if pp > max_pp or tp * pp > total_gpus:
                    continue
                
                dp = total_gpus // (tp * pp)
                if dp < 1 or tp * pp * dp != total_gpus:
                    continue
                
                # EP æœç´¢
                ep_candidates = [1]
                if num_experts > 1:
                    for ep in [2, 4, 8, 16, 32]:
                        if ep <= num_experts and ep <= total_gpus:
                            ep_candidates.append(ep)
                
                for ep in ep_candidates:
                    # Sharding æœç´¢
                    for sharding in ["stage1", "stage2"]:
                        configs.append({
                            "tp": tp,
                            "pp": pp,
                            "dp": dp,
                            "ep": ep,
                            "sharding": sharding,
                        })
        
        return configs
    
    def search_best_throughput(self,
                               total_gpus: int = 8,
                               seq_lens: List[int] = None,
                               micro_batch_sizes: List[int] = None,
                               gas_values: List[int] = None,
                               use_offload: bool = True,
                               offload_ratio: float = 0.95,
                               top_k: int = 10,
                               sort_by: str = "throughput") -> List[Dict]:
        """
        æœç´¢æœ€ä¼˜ååé‡é…ç½®
        
        éå†ä¸åŒçš„å¹¶è¡Œé…ç½®å’Œè®­ç»ƒè¶…å‚æ•°ç»„åˆï¼Œæ‰¾å‡ºæ»¡è¶³æ˜¾å­˜çº¦æŸä¸”ååé‡æœ€é«˜çš„é…ç½®ã€‚
        
        Args:
            total_gpus: æ€» GPU æ•°
            seq_lens: åºåˆ—é•¿åº¦åˆ—è¡¨ (é»˜è®¤ [2048, 4096, 8192])
            micro_batch_sizes: micro batch size åˆ—è¡¨ (é»˜è®¤ [1, 2])
            gas_values: gradient accumulation steps åˆ—è¡¨ (é»˜è®¤ [8, 16, 32])
            use_offload: æ˜¯å¦ä½¿ç”¨ tensorwise offload
            offload_ratio: offload æ¯”ä¾‹
            top_k: è¿”å›å‰ k ä¸ªæœ€ä¼˜é…ç½®
            sort_by: æ’åºä¾æ® ("throughput" æˆ– "step_time")
        
        Returns:
            æ’åºåçš„é…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«å®Œæ•´çš„é…ç½®å’Œé¢„æµ‹ç»“æœ
        """
        if seq_lens is None:
            seq_lens = [2048, 4096, 8192]
        if micro_batch_sizes is None:
            micro_batch_sizes = [1, 2]
        if gas_values is None:
            gas_values = [8, 16, 32]
        
        results = []
        num_experts = self.model_config.num_experts
        
        # ç”Ÿæˆå¹¶è¡Œé…ç½®æœç´¢ç©ºé—´
        parallel_configs = []
        for tp in [1, 2, 4, 8]:
            if tp > total_gpus:
                continue
            for pp in [1, 2, 4]:
                if tp * pp > total_gpus:
                    continue
                dp = total_gpus // (tp * pp)
                if dp < 1 or tp * pp * dp != total_gpus:
                    continue
                
                # å…³é”®çº¦æŸï¼šå½“ä½¿ç”¨ tensorwise_offload æ—¶ï¼ŒDP å¿…é¡» > 1
                # å› ä¸º offload ä¾èµ– Sharding æœºåˆ¶ï¼ŒDP=1 æ—¶ Sharding ä¸å·¥ä½œ
                if use_offload and dp <= 1:
                    continue
                
                # EP å€™é€‰
                ep_candidates = [1]
                if num_experts > 1:
                    for ep in [2, 4, 8]:
                        if ep <= num_experts and ep <= dp:
                            ep_candidates.append(ep)
                
                for ep in ep_candidates:
                    for sharding in ["stage1", "stage2"]:
                        parallel_configs.append({
                            "tp": tp, "pp": pp, "dp": dp, "ep": ep, "sharding": sharding
                        })
        
        print(f"\nğŸ” æœç´¢é…ç½®ç©ºé—´...")
        print(f"   å¹¶è¡Œé…ç½®æ•°: {len(parallel_configs)}")
        print(f"   seq_lens: {seq_lens}")
        print(f"   micro_batch_sizes: {micro_batch_sizes}")
        print(f"   gas_values: {gas_values}")
        print(f"   æ€»ç»„åˆæ•°: {len(parallel_configs) * len(seq_lens) * len(micro_batch_sizes) * len(gas_values)}")
        
        # éå†æ‰€æœ‰ç»„åˆ
        for pcfg in parallel_configs:
            parallel = ParallelConfig.from_dict(pcfg)
            
            for seq_len in seq_lens:
                for mbs in micro_batch_sizes:
                    for gas in gas_values:
                        try:
                            result = self.predict_calibrated(
                                parallel,
                                micro_batch_size=mbs,
                                seq_len=seq_len,
                                gradient_accumulation_steps=gas,
                                tensorwise_offload_optimizer=use_offload,
                                tensorwise_offload_ratio=offload_ratio
                            )
                            
                            # è®¡ç®—å…¨å±€ batch size
                            global_bs = mbs * gas * parallel.dp
                            
                            results.append({
                                "rank": 0,
                                "parallel": pcfg,
                                "parallel_str": str(parallel),
                                "seq_len": seq_len,
                                "micro_batch_size": mbs,
                                "gradient_accumulation_steps": gas,
                                "global_batch_size": global_bs,
                                "step_time_s": result.step_time_ms / 1000,
                                "memory_gb": result.memory_gb,
                                "fits_memory": result.fits_memory,
                                "mfu": result.mfu,
                                "tokens_per_second": result.tokens_per_second,
                                "tokens_per_second_per_gpu": result.tokens_per_second_per_gpu,
                                "tokens_per_step": result.tokens_per_step,
                            })
                        except Exception as e:
                            continue
        
        # è¿‡æ»¤æ»¡è¶³æ˜¾å­˜çº¦æŸçš„é…ç½®
        valid_results = [r for r in results if r["fits_memory"]]
        
        # æ’åº
        if sort_by == "throughput":
            valid_results.sort(key=lambda x: -x["tokens_per_second_per_gpu"])
        else:
            valid_results.sort(key=lambda x: x["step_time_s"])
        
        # æ›´æ–°æ’å
        for i, r in enumerate(valid_results):
            r["rank"] = i + 1
        
        # æ‰“å°æŠ¥å‘Š
        self._print_throughput_report(valid_results[:top_k], total_gpus)
        
        return valid_results[:top_k]
    
    def _print_throughput_report(self, results: List[Dict], total_gpus: int):
        """æ‰“å°ååé‡æ’åºæŠ¥å‘Š"""
        if not results:
            print("\nâŒ æ²¡æœ‰æ‰¾åˆ°æ»¡è¶³æ˜¾å­˜çº¦æŸçš„é…ç½®ï¼")
            return
        
        print("\n" + "=" * 140)
        print(f"ğŸš€ PDCostModel - {total_gpus}å¡æœ€ä¼˜ååé‡é…ç½® Top {len(results)}")
        print("=" * 140)
        print(f"{'æ’å':<4} {'å¹¶è¡Œé…ç½®':<28} {'seq_len':<8} {'mbs':<4} {'gas':<4} "
              f"{'step(s)':<9} {'æ˜¾å­˜(GB)':<10} {'tok/s/GPU':<12} {'global_bs':<10}")
        print("-" * 140)
        
        for r in results:
            print(f"{r['rank']:<4} {r['parallel_str']:<28} {r['seq_len']:<8} "
                  f"{r['micro_batch_size']:<4} {r['gradient_accumulation_steps']:<4} "
                  f"{r['step_time_s']:<9.2f} {r['memory_gb']:<10.1f} "
                  f"{r['tokens_per_second_per_gpu']:<12,.0f} {r['global_batch_size']:<10}")
        
        print("-" * 140)
        
        if results:
            best = results[0]
            print(f"\nğŸ† æœ€ä¼˜é…ç½®:")
            print(f"   å¹¶è¡Œ: {best['parallel_str']}")
            print(f"   seq_len={best['seq_len']}, mbs={best['micro_batch_size']}, gas={best['gradient_accumulation_steps']}")
            print(f"   é¢„è®¡æ—¶å»¶: {best['step_time_s']:.2f} s")
            print(f"   æ˜¾å­˜å ç”¨: {best['memory_gb']:.1f} GB")
            print(f"   ååé‡: {best['tokens_per_second_per_gpu']:,.0f} tok/s/GPU")
            print(f"   Global Batch Size: {best['global_batch_size']}")
    
    def generate_yaml_config(self, config: Dict, output_path: str = None) -> str:
        """
        æ ¹æ®æœç´¢ç»“æœç”Ÿæˆ YAML é…ç½®æ–‡ä»¶
        
        Args:
            config: search_best_throughput è¿”å›çš„é…ç½®å­—å…¸
            output_path: è¾“å‡ºè·¯å¾„ (å¯é€‰)
        
        Returns:
            YAML é…ç½®å†…å®¹å­—ç¬¦ä¸²
        """
        yaml_content = f'''## Qwen3-30B-A3B-Base è‡ªåŠ¨ç”Ÿæˆé…ç½® ##
## é…ç½®: seq_len={config['seq_len']}, mbs={config['micro_batch_size']}, gas={config['gradient_accumulation_steps']} ##
## é¢„æµ‹ååé‡: {config['tokens_per_second_per_gpu']:.0f} tok/s/GPU ##

## data
train_dataset_type: erniekit
eval_dataset_type: erniekit
train_dataset_path: ./data/pt/train.jsonl
train_dataset_prob: "1.0"
eval_dataset_path: ./data/pt/eval.jsonl
eval_dataset_prob: "1.0"

eval_iters: 10
max_seq_len: {config['seq_len']}
num_samples_each_epoch: 6000000
packing: true
mix_strategy: concat
truncate_packing: true
dataloader_shuffle: false
dataloader_num_workers: 8
prefetch_factor: 4

### model
model_name_or_path: /root/paddlejob/workspace/env_run/zhangdongqi/Qwen3-30B-A3B-Base
_attn_implementation: flashmask
use_qk_norm: true

### benchmark é…ç½®
stage: PT
fine_tuning: full
seed: 23
do_train: true
do_eval: false
per_device_eval_batch_size: 1
per_device_train_batch_size: {config['micro_batch_size']}
num_train_epochs: 1
max_steps: 30
eval_steps: 100
evaluation_strategy: steps
save_steps: 999999
save_total_limit: 0
save_strategy: "no"
logging_steps: 1
gradient_accumulation_steps: {config['gradient_accumulation_steps']}
logging_dir: ./log_benchmark_auto
output_dir: ./benchmark_output_auto
disable_tqdm: true
eval_accumulation_steps: 16

# train warmup
warmup_steps: 5
learning_rate: 1.0e-5

# performance - å¹¶è¡Œé…ç½®
tensor_model_parallel_size: {config['parallel']['tp']}
sequence_parallel: {'true' if config['parallel']['tp'] > 1 else 'false'}
pipeline_model_parallel_size: {config['parallel']['pp']}
use_expert_parallel: {'true' if config['parallel']['ep'] > 1 else 'false'}
expert_model_parallel_size: {config['parallel']['ep']}

# recompute
recompute_granularity: full
recompute_method: uniform
recompute_num_layers: 1
using_sonic_moe: false

# sharding
sharding: {config['parallel']['sharding']}
split_param: true
stage1_overlap: true
sd_release_grads: true

apply_rope_fusion: true
moe_grouped_gemm: true
moe_ep_barrier: false
moe_router_fusion: true
moe_router_force_load_balancing: false

# ppé…ç½®
pp_delay_scale_loss: true
overlap_p2p_comm: true
variable_seq_lengths: true
best_unbalanced_scheduler: true
tp_delay_scale_loss: true

optim: adamw
bf16: true
fp16_opt_level: O2
amp_master_grad: true

# checkpoint
save_checkpoint_format: "flex_checkpoint"
load_checkpoint_format: "flex_checkpoint"
tensorwise_offload_optimizer: true
benchmark: true
continue_training: false
'''
        
        if output_path:
            with open(output_path, 'w') as f:
                f.write(yaml_content)
            print(f"\nâœ… é…ç½®å·²ä¿å­˜åˆ°: {output_path}")
        
        return yaml_content

    def save_config(self, path: str):
        """ä¿å­˜é…ç½®"""
        config_data = {
            "model": {
                "num_hidden_layers": self.model_config.num_hidden_layers,
                "hidden_size": self.model_config.hidden_size,
                "intermediate_size": self.model_config.intermediate_size,
                "num_attention_heads": self.model_config.num_attention_heads,
                "num_key_value_heads": self.model_config.num_key_value_heads,
                "num_experts": self.model_config.num_experts,
                "num_experts_per_tok": self.model_config.num_experts_per_tok,
                "vocab_size": self.model_config.vocab_size,
            },
            "hardware": {
                "gpu_name": self.hardware_config.gpu.name,
                "gpu_memory_gb": self.hardware_config.gpu.memory_gb,
                "bf16_tflops": self.hardware_config.gpu.bf16_tflops,
                "num_nodes": self.hardware_config.num_nodes,
                "gpus_per_node": self.hardware_config.gpus_per_node,
            },
            "training": self.training_config.to_dict(),
        }
        
        os.makedirs(os.path.dirname(path) if os.path.dirname(path) else ".", exist_ok=True)
        with open(path, 'w') as f:
            json.dump(config_data, f, indent=2)
    
    @classmethod
    def from_config_file(cls, path: str) -> "PDCostModel":
        """ä»é…ç½®æ–‡ä»¶åŠ è½½"""
        with open(path, 'r') as f:
            data = json.load(f)
        
        model_config = ModelConfig.from_dict(data.get("model", {}))
        
        hw_data = data.get("hardware", {})
        hardware_config = HardwareConfig(
            gpu=GPUSpec.from_name(hw_data.get("gpu_name", "H100-80GB-HBM3")),
            num_nodes=hw_data.get("num_nodes", 1),
            gpus_per_node=hw_data.get("gpus_per_node", 8),
        )
        
        training_config = TrainingConfig.from_dict(data.get("training", {}))
        
        return cls(model_config, hardware_config, training_config)


# ==================== ä¾¿æ·å‡½æ•° ====================

def create_qwen3_30b_costmodel(gpu_memory_gb: float = 80.0,
                               num_nodes: int = 1,
                               gpus_per_node: int = 8) -> PDCostModel:
    """
    åˆ›å»º Qwen3-30B-A3B çš„ CostModel
    """
    model_config = ModelConfig.from_name("qwen3-30b-a3b")
    hardware_config = HardwareConfig(
        gpu=GPUSpec(memory_gb=gpu_memory_gb),
        num_nodes=num_nodes,
        gpus_per_node=gpus_per_node,
    )
    
    return PDCostModel(model_config, hardware_config)


def create_deepseek_v3_costmodel(gpu_memory_gb: float = 80.0,
                                 num_nodes: int = 1,
                                 gpus_per_node: int = 8) -> PDCostModel:
    """
    åˆ›å»º DeepSeek-V3 çš„ CostModel
    """
    model_config = ModelConfig.from_name("deepseek-v3")
    hardware_config = HardwareConfig(
        gpu=GPUSpec(memory_gb=gpu_memory_gb),
        num_nodes=num_nodes,
        gpus_per_node=gpus_per_node,
    )
    
    return PDCostModel(model_config, hardware_config)