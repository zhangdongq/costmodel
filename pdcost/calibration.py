#!/usr/bin/env python3
"""
ç¡¬ä»¶æ ¡å‡†æ¨¡å— - é€šè¿‡å®é™…æµ‹è¯•æ ¡å‡† GPU ç®—åŠ›å’Œé€šä¿¡å¸¦å®½

åŠŸèƒ½:
1. è‡ªåŠ¨æ£€æµ‹ GPU å‹å·å’Œæ˜¾å­˜
2. GEMM benchmark æµ‹è¯•å®é™…ç®—åŠ›
3. NCCL é€šä¿¡å¸¦å®½æµ‹è¯•
4. è‡ªåŠ¨æ›´æ–° HardwareConfig
"""

import os
import time
import subprocess
from dataclasses import dataclass
from typing import Dict, Optional, Tuple, List

# å°è¯•å¯¼å…¥ paddleï¼Œå¦‚æœå¤±è´¥åˆ™æ ‡è®°ä¸ºä¸å¯ç”¨
_PADDLE_AVAILABLE = False
try:
    import paddle
    _PADDLE_AVAILABLE = True
except ImportError:
    pass

from .config import GPUSpec, NetworkSpec, HardwareConfig


@dataclass
class PerformancePoint:
    """å•ä¸ªæ€§èƒ½æµ‹è¯•ç‚¹"""
    size: int  # çŸ©é˜µå°ºå¯¸ (M=N=K)
    tflops: float  # å®æµ‹ TFLOPS
    efficiency: float  # æ•ˆç‡ (å®æµ‹/ç†è®ºå³°å€¼)
    time_ms: float  # å®æµ‹æ—¶é—´


@dataclass 
class PerformanceCurve:
    """æ€§èƒ½æ›²çº¿ï¼ˆå¤šå°ºå¯¸æµ‹è¯•ç»“æœï¼‰"""
    dtype: str  # æ•°æ®ç±»å‹
    points: List[PerformancePoint]  # æµ‹è¯•ç‚¹åˆ—è¡¨
    peak_tflops: float  # å³°å€¼ TFLOPS
    
    # æ‹Ÿåˆå‚æ•° (efficiency = a * log(size) + bï¼Œå¸¦é¥±å’Œä¸Šé™)
    fit_a: float = 0.0  # å¯¹æ•°ç³»æ•°
    fit_b: float = 0.0  # å¸¸æ•°é¡¹
    fit_max: float = 1.0  # æœ€å¤§æ•ˆç‡ä¸Šé™
    fit_min_size: int = 0  # æœ€å°æœ‰æ•ˆå°ºå¯¸
    
    def predict_efficiency(self, size: int) -> float:
        """æ ¹æ®æ‹Ÿåˆæ›²çº¿é¢„æµ‹ç»™å®šå°ºå¯¸çš„æ•ˆç‡"""
        import math
        if size <= 0:
            return 0.0
        
        # å¯¹æ•°æ‹Ÿåˆ: efficiency = a * log(size) + b
        # å¸¦é¥±å’Œä¸Šé™å’Œä¸‹é™
        log_size = math.log(max(size, 1))
        efficiency = self.fit_a * log_size + self.fit_b
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´
        efficiency = max(0.01, min(self.fit_max, efficiency))
        return efficiency
    
    def predict_tflops(self, size: int) -> float:
        """é¢„æµ‹ç»™å®šå°ºå¯¸çš„ TFLOPS"""
        efficiency = self.predict_efficiency(size)
        return self.peak_tflops * efficiency
    
    def to_dict(self) -> Dict:
        return {
            "dtype": self.dtype,
            "peak_tflops": round(self.peak_tflops, 2),
            "fit_a": round(self.fit_a, 6),
            "fit_b": round(self.fit_b, 6),
            "fit_max": round(self.fit_max, 4),
            "points": [
                {"size": p.size, "tflops": round(p.tflops, 2), 
                 "efficiency": round(p.efficiency, 4), "time_ms": round(p.time_ms, 3)}
                for p in self.points
            ]
        }
    
    def __str__(self) -> str:
        lines = [f"PerformanceCurve ({self.dtype}):"]
        lines.append(f"  Peak: {self.peak_tflops:.1f} TFLOPS")
        lines.append(f"  Fit: eff = {self.fit_a:.4f} * log(size) + {self.fit_b:.4f}")
        lines.append(f"  Max Efficiency: {self.fit_max:.1%}")
        lines.append(f"  Test Points:")
        for p in self.points:
            lines.append(f"    {p.size:>6}: {p.tflops:>7.1f} TFLOPS ({p.efficiency:>5.1%})")
        return "\n".join(lines)


@dataclass
class CalibrationResult:
    """æ ¡å‡†ç»“æœ"""
    # GPU ä¿¡æ¯
    gpu_name: str = "Unknown"
    gpu_memory_gb: float = 0.0
    gpu_count: int = 0
    
    # å®æµ‹ç®—åŠ› (TFLOPS) - å³°å€¼ï¼ˆå¤§çŸ©é˜µï¼‰
    fp32_tflops: float = 0.0
    fp16_tflops: float = 0.0
    bf16_tflops: float = 0.0
    
    # å®æµ‹å¸¦å®½ (GB/s)
    memory_bandwidth_gbps: float = 0.0
    intra_node_bandwidth_gbps: float = 0.0
    
    # æ€§èƒ½æ›²çº¿ï¼ˆå¤šå°ºå¯¸æµ‹è¯•ï¼‰
    fp32_curve: Optional[PerformanceCurve] = None
    fp16_curve: Optional[PerformanceCurve] = None
    bf16_curve: Optional[PerformanceCurve] = None
    
    # æ ¡å‡†çŠ¶æ€
    calibrated: bool = False
    error_message: str = ""
    
    def get_efficiency(self, size: int, dtype: str = "bfloat16") -> float:
        """æ ¹æ®æ•°æ®å°ºå¯¸å’Œç±»å‹è·å–é¢„æµ‹æ•ˆç‡"""
        curve = None
        if dtype in ["float32", "fp32"]:
            curve = self.fp32_curve
        elif dtype in ["float16", "fp16"]:
            curve = self.fp16_curve
        elif dtype in ["bfloat16", "bf16"]:
            curve = self.bf16_curve
        
        if curve is not None:
            return curve.predict_efficiency(size)
        
        # é»˜è®¤æ•ˆç‡ä¼°ç®—ï¼ˆæ— æ›²çº¿æ—¶ï¼‰
        import math
        base_eff = 0.5
        size_factor = min(1.0, math.log(max(size, 64)) / math.log(8192))
        return base_eff * size_factor
    
    def to_dict(self) -> Dict:
        result = {
            "gpu_name": self.gpu_name,
            "gpu_memory_gb": round(self.gpu_memory_gb, 2),
            "gpu_count": self.gpu_count,
            "fp32_tflops": round(self.fp32_tflops, 2),
            "fp16_tflops": round(self.fp16_tflops, 2),
            "bf16_tflops": round(self.bf16_tflops, 2),
            "memory_bandwidth_gbps": round(self.memory_bandwidth_gbps, 2),
            "intra_node_bandwidth_gbps": round(self.intra_node_bandwidth_gbps, 2),
            "calibrated": self.calibrated,
        }
        if self.bf16_curve:
            result["bf16_curve"] = self.bf16_curve.to_dict()
        if self.fp16_curve:
            result["fp16_curve"] = self.fp16_curve.to_dict()
        if self.fp32_curve:
            result["fp32_curve"] = self.fp32_curve.to_dict()
        return result
    
    def __str__(self) -> str:
        if not self.calibrated:
            return f"CalibrationResult: Not calibrated ({self.error_message})"
        
        lines = [
            f"CalibrationResult:",
            f"  GPU: {self.gpu_name} Ã— {self.gpu_count}",
            f"  Memory: {self.gpu_memory_gb:.1f} GB",
            f"  FP32 Peak: {self.fp32_tflops:.1f} TFLOPS",
            f"  FP16 Peak: {self.fp16_tflops:.1f} TFLOPS",
            f"  BF16 Peak: {self.bf16_tflops:.1f} TFLOPS",
            f"  Memory BW: {self.memory_bandwidth_gbps:.1f} GB/s",
        ]
        
        if self.bf16_curve:
            lines.append(f"\n{self.bf16_curve}")
        
        return "\n".join(lines)


class HardwareCalibrator:
    """
    ç¡¬ä»¶æ ¡å‡†å™¨
    
    é€šè¿‡å®é™…è¿è¡Œ benchmark æµ‹è¯•ç¡¬ä»¶æ€§èƒ½
    """
    
    def __init__(self, device_id: int = 0, warmup_iters: int = 5, test_iters: int = 20):
        """
        Args:
            device_id: æµ‹è¯•ä½¿ç”¨çš„ GPU ID
            warmup_iters: é¢„çƒ­è¿­ä»£æ¬¡æ•°
            test_iters: æµ‹è¯•è¿­ä»£æ¬¡æ•°
        """
        self.device_id = device_id
        self.warmup_iters = warmup_iters
        self.test_iters = test_iters
        self._result: Optional[CalibrationResult] = None
    
    @property
    def result(self) -> Optional[CalibrationResult]:
        """è·å–æ ¡å‡†ç»“æœ"""
        return self._result
    
    def detect_gpu_info(self) -> Tuple[str, float, int]:
        """
        æ£€æµ‹ GPU ä¿¡æ¯
        
        Returns:
            (gpu_name, memory_gb, gpu_count)
        """
        gpu_name = "Unknown"
        memory_gb = 0.0
        gpu_count = 0
        
        # æ–¹æ³•1: ä½¿ç”¨ nvidia-smi
        try:
            result = subprocess.run(
                ["nvidia-smi", "--query-gpu=name,memory.total,count", 
                 "--format=csv,noheader,nounits"],
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                gpu_count = len(lines)
                if lines:
                    parts = lines[0].split(', ')
                    gpu_name = parts[0].strip()
                    memory_gb = float(parts[1]) / 1024  # MB to GB
        except Exception:
            pass
        
        # æ–¹æ³•2: ä½¿ç”¨ paddle
        if _PADDLE_AVAILABLE and gpu_count == 0:
            try:
                gpu_count = paddle.device.cuda.device_count()
                if gpu_count > 0:
                    props = paddle.device.cuda.get_device_properties(self.device_id)
                    gpu_name = props.name
                    memory_gb = props.total_memory / (1024 ** 3)
            except Exception:
                pass
        
        return gpu_name, memory_gb, gpu_count
    
    def benchmark_gemm(self, m: int, n: int, k: int, dtype: str = "float32",
                       warmup: int = None, iters: int = None) -> float:
        """
        GEMM ç®—åŠ›æµ‹è¯•
        
        Args:
            m, n, k: çŸ©é˜µå°ºå¯¸
            dtype: æ•°æ®ç±»å‹ (float32, float16, bfloat16)
            warmup: é¢„çƒ­æ¬¡æ•°
            iters: æµ‹è¯•æ¬¡æ•°
        
        Returns:
            å®æµ‹ TFLOPS
        """
        if not _PADDLE_AVAILABLE:
            return 0.0
        
        warmup = warmup or self.warmup_iters
        iters = iters or self.test_iters
        
        try:
            paddle.set_device(f'gpu:{self.device_id}')
            
            # åˆ›å»ºæµ‹è¯•çŸ©é˜µ
            dtype_map = {
                "float32": paddle.float32,
                "float16": paddle.float16,
                "bfloat16": paddle.bfloat16,
            }
            pd_dtype = dtype_map.get(dtype, paddle.float32)
            
            a = paddle.randn([m, k], dtype=pd_dtype)
            b = paddle.randn([k, n], dtype=pd_dtype)
            
            # é¢„çƒ­
            for _ in range(warmup):
                c = paddle.matmul(a, b)
                paddle.device.cuda.synchronize()
            
            # è®¡æ—¶æµ‹è¯•
            start_time = time.perf_counter()
            for _ in range(iters):
                c = paddle.matmul(a, b)
            paddle.device.cuda.synchronize()
            end_time = time.perf_counter()
            
            # è®¡ç®— TFLOPS
            elapsed_ms = (end_time - start_time) * 1000 / iters
            flops = 2 * m * n * k  # GEMM: 2*M*N*K FLOPs
            tflops = flops / (elapsed_ms / 1000) / 1e12
            
            # æ¸…ç†
            del a, b, c
            paddle.device.cuda.empty_cache()
            
            return tflops
            
        except Exception as e:
            print(f"GEMM benchmark failed: {e}")
            return 0.0
    
    def benchmark_gemm_with_time(self, m: int, n: int, k: int, dtype: str = "float32",
                                  warmup: int = None, iters: int = None) -> Tuple[float, float]:
        """
        GEMM ç®—åŠ›æµ‹è¯•ï¼Œè¿”å› TFLOPS å’Œæ—¶é—´
        
        Returns:
            (tflops, elapsed_ms)
        """
        if not _PADDLE_AVAILABLE:
            return 0.0, 0.0
        
        warmup = warmup or self.warmup_iters
        iters = iters or self.test_iters
        
        try:
            paddle.set_device(f'gpu:{self.device_id}')
            
            dtype_map = {
                "float32": paddle.float32,
                "float16": paddle.float16,
                "bfloat16": paddle.bfloat16,
            }
            pd_dtype = dtype_map.get(dtype, paddle.float32)
            
            a = paddle.randn([m, k], dtype=pd_dtype)
            b = paddle.randn([k, n], dtype=pd_dtype)
            
            # é¢„çƒ­
            for _ in range(warmup):
                c = paddle.matmul(a, b)
                paddle.device.cuda.synchronize()
            
            # è®¡æ—¶æµ‹è¯•
            start_time = time.perf_counter()
            for _ in range(iters):
                c = paddle.matmul(a, b)
            paddle.device.cuda.synchronize()
            end_time = time.perf_counter()
            
            # è®¡ç®—
            elapsed_ms = (end_time - start_time) * 1000 / iters
            flops = 2 * m * n * k
            tflops = flops / (elapsed_ms / 1000) / 1e12
            
            del a, b, c
            paddle.device.cuda.empty_cache()
            
            return tflops, elapsed_ms
            
        except Exception as e:
            print(f"GEMM benchmark failed for size {m}: {e}")
            return 0.0, 0.0
    
    def benchmark_gemm_multi_size(self, dtype: str = "bfloat16",
                                   sizes: List[int] = None,
                                   theoretical_peak: float = None,
                                   verbose: bool = True) -> PerformanceCurve:
        """
        å¤šå°ºå¯¸ GEMM æµ‹è¯•ï¼Œç”Ÿæˆæ€§èƒ½æ›²çº¿
        
        Args:
            dtype: æ•°æ®ç±»å‹
            sizes: æµ‹è¯•å°ºå¯¸åˆ—è¡¨ (é»˜è®¤ä» 64 åˆ° 16384 çš„ 12 ä¸ªå°ºå¯¸)
            theoretical_peak: ç†è®ºå³°å€¼ TFLOPS (ç”¨äºè®¡ç®—æ•ˆç‡)
            verbose: æ˜¯å¦æ‰“å°è¿›åº¦
        
        Returns:
            PerformanceCurve: æ€§èƒ½æ›²çº¿
        """
        import math
        
        # é»˜è®¤æµ‹è¯•å°ºå¯¸ï¼šä»å°åˆ°å¤§ 12 ä¸ªç‚¹
        if sizes is None:
            sizes = [64, 128, 256, 512, 768, 1024, 1536, 2048, 3072, 4096, 6144, 8192]
        
        if verbose:
            print(f"\nğŸ“Š å¤šå°ºå¯¸ GEMM æµ‹è¯• ({dtype}):")
            print(f"  æµ‹è¯•å°ºå¯¸: {sizes}")
        
        points = []
        max_tflops = 0.0
        
        for size in sizes:
            tflops, time_ms = self.benchmark_gemm_with_time(size, size, size, dtype)
            
            if tflops > max_tflops:
                max_tflops = tflops
            
            # æ•ˆç‡è®¡ç®—ï¼ˆå¦‚æœæœ‰ç†è®ºå³°å€¼ï¼‰
            efficiency = tflops / theoretical_peak if theoretical_peak and theoretical_peak > 0 else 0.0
            
            point = PerformancePoint(
                size=size,
                tflops=tflops,
                efficiency=efficiency,
                time_ms=time_ms
            )
            points.append(point)
            
            if verbose:
                eff_str = f"({efficiency:.1%})" if efficiency > 0 else ""
                print(f"    {size:>6}: {tflops:>7.1f} TFLOPS {eff_str}")
        
        # ä½¿ç”¨å®æµ‹æœ€å¤§å€¼ä½œä¸ºå³°å€¼ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ç†è®ºå³°å€¼ï¼‰
        peak = theoretical_peak if theoretical_peak and theoretical_peak > 0 else max_tflops
        
        # æ›´æ–°æ•ˆç‡
        for p in points:
            if peak > 0:
                p.efficiency = p.tflops / peak
        
        # åˆ›å»ºæ›²çº¿å¹¶æ‹Ÿåˆ
        curve = PerformanceCurve(
            dtype=dtype,
            points=points,
            peak_tflops=peak
        )
        
        # æ‹Ÿåˆæ›²çº¿
        self._fit_curve(curve)
        
        if verbose:
            print(f"\n  å³°å€¼: {peak:.1f} TFLOPS")
            print(f"  æ‹Ÿåˆå…¬å¼: efficiency = {curve.fit_a:.4f} * log(size) + {curve.fit_b:.4f}")
            print(f"  æœ€å¤§æ•ˆç‡: {curve.fit_max:.1%}")
        
        return curve
    
    def _fit_curve(self, curve: PerformanceCurve):
        """
        æ‹Ÿåˆæ€§èƒ½æ›²çº¿
        
        ä½¿ç”¨å¯¹æ•°æ¨¡å‹: efficiency = a * log(size) + b
        ä½¿ç”¨æœ€å°äºŒä¹˜æ³•æ‹Ÿåˆ
        """
        import math
        
        if not curve.points:
            return
        
        # å‡†å¤‡æ•°æ®
        x = []  # log(size)
        y = []  # efficiency
        
        for p in curve.points:
            if p.size > 0 and p.efficiency > 0:
                x.append(math.log(p.size))
                y.append(p.efficiency)
        
        if len(x) < 2:
            return
        
        # æœ€å°äºŒä¹˜æ³•æ‹Ÿåˆ y = a*x + b
        n = len(x)
        sum_x = sum(x)
        sum_y = sum(y)
        sum_xy = sum(xi * yi for xi, yi in zip(x, y))
        sum_xx = sum(xi * xi for xi in x)
        
        # è®¡ç®—æ–œç‡ a å’Œæˆªè· b
        denominator = n * sum_xx - sum_x * sum_x
        if abs(denominator) < 1e-10:
            curve.fit_a = 0
            curve.fit_b = sum_y / n if n > 0 else 0
        else:
            curve.fit_a = (n * sum_xy - sum_x * sum_y) / denominator
            curve.fit_b = (sum_y - curve.fit_a * sum_x) / n
        
        # è®¡ç®—æœ€å¤§æ•ˆç‡ï¼ˆå–å®æµ‹æœ€å¤§å€¼æˆ–æ‹Ÿåˆæœ€å¤§å€¼çš„è¾ƒå°è€…ï¼‰
        max_measured = max(p.efficiency for p in curve.points)
        max_fitted = curve.fit_a * math.log(16384) + curve.fit_b  # åœ¨ 16384 å°ºå¯¸å¤„çš„æ‹Ÿåˆå€¼
        curve.fit_max = min(max_measured * 1.05, max_fitted, 1.0)  # ä¸Šé™ 100%
        
        # æ‰¾åˆ°æœ€å°æœ‰æ•ˆå°ºå¯¸ï¼ˆæ•ˆç‡ > 1%ï¼‰
        for size in [32, 64, 128, 256]:
            eff = curve.fit_a * math.log(size) + curve.fit_b
            if eff > 0.01:
                curve.fit_min_size = size
                break

    def benchmark_memory_bandwidth(self, size_mb: int = 256) -> float:
        """
        æ˜¾å­˜å¸¦å®½æµ‹è¯•
        
        Args:
            size_mb: æµ‹è¯•æ•°æ®å¤§å° (MB)
        
        Returns:
            å®æµ‹å¸¦å®½ (GB/s)
        """
        if not _PADDLE_AVAILABLE:
            return 0.0
        
        try:
            paddle.set_device(f'gpu:{self.device_id}')
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            num_elements = size_mb * 1024 * 1024 // 4  # float32
            src = paddle.randn([num_elements], dtype=paddle.float32)
            
            # é¢„çƒ­
            for _ in range(self.warmup_iters):
                dst = src.clone()
                paddle.device.cuda.synchronize()
            
            # è®¡æ—¶æµ‹è¯•
            start_time = time.perf_counter()
            for _ in range(self.test_iters):
                dst = src.clone()
            paddle.device.cuda.synchronize()
            end_time = time.perf_counter()
            
            # è®¡ç®—å¸¦å®½ (è¯» + å†™)
            elapsed_s = (end_time - start_time) / self.test_iters
            data_gb = size_mb / 1024 * 2  # è¯» + å†™
            bandwidth_gbps = data_gb / elapsed_s
            
            # æ¸…ç†
            del src, dst
            paddle.device.cuda.empty_cache()
            
            return bandwidth_gbps
            
        except Exception as e:
            print(f"Memory bandwidth benchmark failed: {e}")
            return 0.0
    
    def calibrate(self, 
                  test_compute: bool = True,
                  test_memory: bool = True,
                  gemm_size: int = 8192,
                  multi_size_test: bool = False,
                  test_sizes: List[int] = None,
                  verbose: bool = True) -> CalibrationResult:
        """
        æ‰§è¡Œå®Œæ•´æ ¡å‡†
        
        Args:
            test_compute: æ˜¯å¦æµ‹è¯•ç®—åŠ›
            test_memory: æ˜¯å¦æµ‹è¯•æ˜¾å­˜å¸¦å®½
            gemm_size: GEMM æµ‹è¯•çŸ©é˜µå¤§å° (ç”¨äºå³°å€¼æµ‹è¯•)
            multi_size_test: æ˜¯å¦è¿›è¡Œå¤šå°ºå¯¸æµ‹è¯•ç”Ÿæˆæ€§èƒ½æ›²çº¿
            test_sizes: å¤šå°ºå¯¸æµ‹è¯•çš„å°ºå¯¸åˆ—è¡¨ (é»˜è®¤ 12 ä¸ªä» 64 åˆ° 8192)
            verbose: æ˜¯å¦æ‰“å°è¿›åº¦
        
        Returns:
            CalibrationResult
        """
        result = CalibrationResult()
        
        if verbose:
            print("=" * 60)
            print("ğŸ”§ pdcost ç¡¬ä»¶æ ¡å‡†")
            print("=" * 60)
        
        # 1. æ£€æµ‹ GPU ä¿¡æ¯
        if verbose:
            print("\n[1/5] æ£€æµ‹ GPU ä¿¡æ¯...")
        
        gpu_name, memory_gb, gpu_count = self.detect_gpu_info()
        result.gpu_name = gpu_name
        result.gpu_memory_gb = memory_gb
        result.gpu_count = gpu_count
        
        if verbose:
            print(f"  GPU: {gpu_name}")
            print(f"  æ˜¾å­˜: {memory_gb:.1f} GB")
            print(f"  æ•°é‡: {gpu_count}")
        
        if gpu_count == 0:
            result.error_message = "No GPU detected"
            return result
        
        if not _PADDLE_AVAILABLE:
            result.error_message = "PaddlePaddle not available"
            if verbose:
                print("\nâš ï¸ PaddlePaddle æœªå®‰è£…ï¼Œä½¿ç”¨é¢„è®¾å€¼")
            # ä½¿ç”¨é¢„è®¾å€¼
            self._use_preset_values(result)
            result.calibrated = True
            self._result = result
            return result
        
        # 2. æµ‹è¯• FP32 å³°å€¼ç®—åŠ›
        if test_compute:
            if verbose:
                print(f"\n[2/5] æµ‹è¯• FP32 å³°å€¼ç®—åŠ› (GEMM {gemm_size}Ã—{gemm_size})...")
            result.fp32_tflops = self.benchmark_gemm(gemm_size, gemm_size, gemm_size, "float32")
            if verbose:
                print(f"  FP32 å³°å€¼: {result.fp32_tflops:.1f} TFLOPS")
        
        # 3. æµ‹è¯• FP16/BF16 å³°å€¼ç®—åŠ›
        if test_compute:
            if verbose:
                print(f"\n[3/5] æµ‹è¯• FP16/BF16 å³°å€¼ç®—åŠ›...")
            
            # FP16
            result.fp16_tflops = self.benchmark_gemm(gemm_size, gemm_size, gemm_size, "float16")
            if verbose:
                print(f"  FP16 å³°å€¼: {result.fp16_tflops:.1f} TFLOPS")
            
            # BF16
            try:
                result.bf16_tflops = self.benchmark_gemm(gemm_size, gemm_size, gemm_size, "bfloat16")
            except Exception:
                result.bf16_tflops = result.fp16_tflops  # é™çº§
            if verbose:
                print(f"  BF16 å³°å€¼: {result.bf16_tflops:.1f} TFLOPS")
        
        # 4. æµ‹è¯•æ˜¾å­˜å¸¦å®½
        if test_memory:
            if verbose:
                print(f"\n[4/5] æµ‹è¯•æ˜¾å­˜å¸¦å®½...")
            result.memory_bandwidth_gbps = self.benchmark_memory_bandwidth(256)
            if verbose:
                print(f"  å¸¦å®½: {result.memory_bandwidth_gbps:.1f} GB/s")
        
        # 5. å¤šå°ºå¯¸æ€§èƒ½æ›²çº¿æµ‹è¯•
        if multi_size_test and test_compute:
            if verbose:
                print(f"\n[5/5] å¤šå°ºå¯¸æ€§èƒ½æ›²çº¿æµ‹è¯•...")
            
            # ä½¿ç”¨é»˜è®¤å°ºå¯¸æˆ–è‡ªå®šä¹‰å°ºå¯¸
            if test_sizes is None:
                test_sizes = [64, 128, 256, 512, 768, 1024, 1536, 2048, 3072, 4096, 6144, 8192]
            
            # BF16 æ€§èƒ½æ›²çº¿
            if result.bf16_tflops > 0:
                result.bf16_curve = self.benchmark_gemm_multi_size(
                    dtype="bfloat16",
                    sizes=test_sizes,
                    theoretical_peak=result.bf16_tflops,
                    verbose=verbose
                )
            
            # FP16 æ€§èƒ½æ›²çº¿ (å¯é€‰)
            if result.fp16_tflops > 0:
                result.fp16_curve = self.benchmark_gemm_multi_size(
                    dtype="float16",
                    sizes=test_sizes,
                    theoretical_peak=result.fp16_tflops,
                    verbose=verbose
                )
        elif verbose:
            print(f"\n[5/5] è·³è¿‡å¤šå°ºå¯¸æµ‹è¯• (multi_size_test=False)")
        
        result.calibrated = True
        self._result = result
        
        if verbose:
            print("\n" + "=" * 60)
            print("âœ… æ ¡å‡†å®Œæˆ!")
            print("=" * 60)
            
            if result.bf16_curve:
                print("\nğŸ“ˆ BF16 æ€§èƒ½æ›²çº¿æ‹Ÿåˆç»“æœ:")
                print(f"  å…¬å¼: efficiency = {result.bf16_curve.fit_a:.4f} * log(size) + {result.bf16_curve.fit_b:.4f}")
                print(f"  å³°å€¼æ•ˆç‡: {result.bf16_curve.fit_max:.1%}")
        
        return result
    
    def _use_preset_values(self, result: CalibrationResult):
        """ä½¿ç”¨é¢„è®¾å€¼ï¼ˆå½“æ— æ³•å®é™…æµ‹è¯•æ—¶ï¼‰"""
        # æ ¹æ® GPU åç§°åŒ¹é…é¢„è®¾
        name_lower = result.gpu_name.lower()
        
        if "h100" in name_lower:
            result.fp32_tflops = 67.0
            result.fp16_tflops = 989.0
            result.bf16_tflops = 989.0
            result.memory_bandwidth_gbps = 3350.0
        elif "a100" in name_lower:
            result.fp32_tflops = 19.5
            result.fp16_tflops = 312.0
            result.bf16_tflops = 312.0
            result.memory_bandwidth_gbps = 2039.0
        elif "a800" in name_lower:
            result.fp32_tflops = 19.5
            result.fp16_tflops = 312.0
            result.bf16_tflops = 312.0
            result.memory_bandwidth_gbps = 2039.0
        elif "v100" in name_lower:
            result.fp32_tflops = 15.7
            result.fp16_tflops = 125.0
            result.bf16_tflops = 0.0
            result.memory_bandwidth_gbps = 900.0
        elif "4090" in name_lower:
            result.fp32_tflops = 82.6
            result.fp16_tflops = 330.0
            result.bf16_tflops = 330.0
            result.memory_bandwidth_gbps = 1008.0
        else:
            # é»˜è®¤å€¼
            result.fp32_tflops = 20.0
            result.fp16_tflops = 100.0
            result.bf16_tflops = 100.0
            result.memory_bandwidth_gbps = 1000.0
    
    def create_hardware_config(self, 
                               num_nodes: int = 1,
                               gpus_per_node: int = None) -> HardwareConfig:
        """
        æ ¹æ®æ ¡å‡†ç»“æœåˆ›å»º HardwareConfig
        
        Args:
            num_nodes: èŠ‚ç‚¹æ•°
            gpus_per_node: æ¯èŠ‚ç‚¹ GPU æ•°ï¼ˆé»˜è®¤ä½¿ç”¨æ£€æµ‹åˆ°çš„æ•°é‡ï¼‰
        
        Returns:
            HardwareConfig
        """
        if self._result is None:
            self.calibrate()
        
        result = self._result
        
        gpu = GPUSpec(
            name=result.gpu_name,
            memory_gb=result.gpu_memory_gb,
            fp32_tflops=result.fp32_tflops,
            fp16_tflops=result.fp16_tflops,
            bf16_tflops=result.bf16_tflops,
            memory_bandwidth_gbps=result.memory_bandwidth_gbps,
        )
        
        if gpus_per_node is None:
            gpus_per_node = result.gpu_count
        
        # ä¼°ç®—ç½‘ç»œå¸¦å®½ï¼ˆåŸºäº GPU å‹å·ï¼‰
        name_lower = result.gpu_name.lower()
        if "h100" in name_lower or "h800" in name_lower:
            intra_bw = 900.0  # NVLink 4.0 (H800 åŒæ ·æ”¯æŒ)
            inter_bw = 200.0  # 8x IB HDR
        elif "a100" in name_lower or "a800" in name_lower:
            intra_bw = 600.0  # NVLink 3.0
            inter_bw = 200.0
        else:
            intra_bw = 300.0
            inter_bw = 100.0
        
        if result.intra_node_bandwidth_gbps > 0:
            intra_bw = result.intra_node_bandwidth_gbps
        
        network = NetworkSpec(
            intra_node_bandwidth_gbps=intra_bw,
            inter_node_bandwidth_gbps=inter_bw,
        )
        
        return HardwareConfig(
            gpu=gpu,
            network=network,
            num_nodes=num_nodes,
            gpus_per_node=gpus_per_node,
        )


def quick_calibrate(device_id: int = 0, verbose: bool = True) -> CalibrationResult:
    """
    å¿«é€Ÿæ ¡å‡†ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        device_id: GPU ID
        verbose: æ˜¯å¦æ‰“å°è¿›åº¦
    
    Returns:
        CalibrationResult
    """
    calibrator = HardwareCalibrator(device_id=device_id)
    return calibrator.calibrate(verbose=verbose)


def create_calibrated_hardware_config(
    num_nodes: int = 1,
    gpus_per_node: int = None,
    device_id: int = 0,
    verbose: bool = True
) -> HardwareConfig:
    """
    åˆ›å»ºç»è¿‡æ ¡å‡†çš„ HardwareConfig
    
    Args:
        num_nodes: èŠ‚ç‚¹æ•°
        gpus_per_node: æ¯èŠ‚ç‚¹ GPU æ•°
        device_id: æµ‹è¯•ä½¿ç”¨çš„ GPU ID
        verbose: æ˜¯å¦æ‰“å°è¿›åº¦
    
    Returns:
        HardwareConfig
    """
    calibrator = HardwareCalibrator(device_id=device_id)
    calibrator.calibrate(verbose=verbose)
    return calibrator.create_hardware_config(num_nodes, gpus_per_node)