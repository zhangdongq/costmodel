

å¥½çš„ï¼Œæˆ‘æ¥å®Œæ•´è®²è§£ `galvatron_costmodel` çš„æ‰€æœ‰ä»£ç ã€‚é¦–å…ˆè¯»å–æ‰€æœ‰æ–‡ä»¶ï¼š



```extract_content_blocks
{"pathes":"galvatron_costmodel/__init__.py\ngalvatron_costmodel/hardware_config.py@LINE[..200]\n"}
```



```extract_content_blocks
{"pathes":"galvatron_costmodel/hardware_config.py@LINE[200..438]\n"}
```



```extract_content_blocks
{"pathes":"galvatron_costmodel/communication_model.py@LINE[..180]\n"}
```



```extract_content_blocks
{"pathes":"galvatron_costmodel/communication_model.py@LINE[180..350]\ngalvatron_costmodel/communication_model.py@LINE[350..537]\n"}
```



```extract_content_blocks
{"pathes":"galvatron_costmodel/computation_model.py@LINE[..250]\n"}
```



```extract_content_blocks
{"pathes":"galvatron_costmodel/computation_model.py@LINE[250..486]\n"}
```



```extract_content_blocks
{"pathes":"galvatron_costmodel/memory_model.py@LINE[..200]\n"}
```



```extract_content_blocks
{"pathes":"galvatron_costmodel/memory_model.py@LINE[200..400]\ngalvatron_costmodel/memory_model.py@LINE[400..581]\n"}
```



ç°åœ¨æˆ‘å·²ç»è¯»å–äº†æ‰€æœ‰ä»£ç ï¼Œè®©æˆ‘ä¸ºä½ å®Œæ•´è®²è§£ `galvatron_costmodel` çš„æ•´ä½“æ¶æ„å’Œæ¯ä¸ªæ¨¡å—çš„è®¾è®¡ã€‚

---

# Galvatron CostModel å®Œæ•´ä»£ç è®²è§£

## ğŸ“¦ æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GalvatronCostModel                           â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚    â”‚ Computationâ”‚  â”‚Communicationâ”‚ â”‚  Memory    â”‚              â”‚
â”‚    â”‚   Model    â”‚  â”‚    Model    â”‚ â”‚   Model    â”‚              â”‚
â”‚    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚          â”‚               â”‚              â”‚                      â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                  â”‚                                              â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚          â”‚ HardwareConfigâ”‚                                      â”‚
â”‚          â”‚ (GPU, Network,â”‚                                      â”‚
â”‚          â”‚   Cluster)    â”‚                                      â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1ï¸âƒ£ `hardware_config.py` - ç¡¬ä»¶é…ç½®æ¨¡å—

### 1.1 GPUSpecs - GPU è§„æ ¼

å®šä¹‰ GPU çš„è®¡ç®—èƒ½åŠ›å’Œæ˜¾å­˜å¸¦å®½ï¼š

```python
@dataclass
class GPUSpecs:
    # è®¡ç®—èƒ½åŠ› (TFLOPS)
    fp16_tflops: float = 989.0      # H800 FP16 å³°å€¼
    bf16_tflops: float = 989.0      # BF16 å³°å€¼ï¼ˆè®­ç»ƒä¸»è¦ç”¨è¿™ä¸ªï¼‰
    
    # æ˜¾å­˜
    memory_gb: float = 80.0               # æ˜¾å­˜å®¹é‡ (GB)
    memory_bandwidth_gbs: float = 3350.0  # æ˜¾å­˜å¸¦å®½ (GB/s)
```

**å•ä½çº¦å®š**ï¼š
- ç®—åŠ›: **TFLOPS** (10^12 FLOPS)
- æ˜¾å­˜: **GB**
- å¸¦å®½: **GB/s**ï¼ˆå­—èŠ‚/ç§’ï¼Œä¸æ˜¯ bit/ç§’ï¼ï¼‰

### 1.2 NetworkTopology - ç½‘ç»œæ‹“æ‰‘

åˆ†å±‚ç½‘ç»œå»ºæ¨¡ï¼ŒåŒºåˆ†èŠ‚ç‚¹å†…ï¼ˆNVLinkï¼‰å’ŒèŠ‚ç‚¹é—´ï¼ˆInfiniBandï¼‰ï¼š

```python
@dataclass
class NetworkTopology:
    # èŠ‚ç‚¹å†…é€šä¿¡ï¼ˆNVLinkï¼‰
    intra_node_bandwidth_gbs: float = 900.0   # 900 GB/s
    intra_node_latency_us: float = 1.0        # 1 Î¼s
    
    # èŠ‚ç‚¹é—´é€šä¿¡ï¼ˆInfiniBandï¼‰
    # 200 Gbps = 200 / 8 = 25 GB/s
    inter_node_bandwidth_gbs: float = 25.0    # 25 GB/s
    inter_node_latency_us: float = 5.0        # 5 Î¼s
    
    # æ•ˆç‡å› å­ (è€ƒè™‘åè®®å¼€é”€ã€æ‹¥å¡ç­‰)
    allreduce_efficiency: float = 0.85
    alltoall_efficiency: float = 0.80
```

**å…³é”®è®¾è®¡**ï¼šæ‰€æœ‰å¸¦å®½ç»Ÿä¸€å­˜å‚¨ä¸º **GB/s**ï¼Œé¿å… Gbps/GB/s æ··æ·†ï¼

### 1.3 ClusterConfig - é›†ç¾¤é…ç½®

```python
@dataclass
class ClusterConfig:
    num_nodes: int = 1                # èŠ‚ç‚¹æ•°
    gpus_per_node: int = 8            # æ¯èŠ‚ç‚¹ GPU æ•°
    
    @property
    def total_gpus(self) -> int:
        return self.num_nodes * self.gpus_per_node
```

---

## 2ï¸âƒ£ `communication_model.py` - é€šä¿¡æ¨¡å‹æ¨¡å—

### 2.1 é€šä¿¡æ—¶é—´å…¬å¼

é€šä¿¡æ—¶é—´ = **å»¶è¿Ÿæ—¶é—´** + **ä¼ è¾“æ—¶é—´**

```
T = Î± Ã— num_steps + data_size / (bandwidth Ã— efficiency)
```

å…¶ä¸­ï¼š
- `Î±` = å•æ¬¡é€šä¿¡å»¶è¿Ÿ (Î¼s)
- `num_steps` = é€šä¿¡æ­¥æ•°ï¼ˆå–å†³äºç®—æ³•å’Œå¹¶è¡Œåº¦ï¼‰
- `data_size` = é€šä¿¡æ•°æ®é‡ (bytes)
- `bandwidth` = å¸¦å®½ (GB/s)
- `efficiency` = æ•ˆç‡å› å­ (0-1)

### 2.2 AllReduce æ¨¡å‹

ç”¨äº **TPï¼ˆå¼ é‡å¹¶è¡Œï¼‰** æ¢¯åº¦åŒæ­¥ï¼š

```python
class AllReduceModel:
    def predict(self, data_size_bytes, num_gpus, is_intra_node):
        # Ring AllReduce é€šä¿¡é‡
        ring_factor = 2 * (num_gpus - 1) / num_gpus
        comm_volume_bytes = data_size_bytes * ring_factor
        
        # å»¶è¿Ÿéƒ¨åˆ†ï¼š2 * (N-1) æ¬¡é€šä¿¡
        num_steps = 2 * (num_gpus - 1)
        latency_ms = num_steps * latency_us / 1000.0
        
        # ä¼ è¾“éƒ¨åˆ†
        transfer_ms = data_size_gb / (bandwidth_gbs * efficiency) * 1000.0
        
        return latency_ms + transfer_ms
```

**Ring AllReduce åŸç†**ï¼š
- æ¯ä¸ª GPU åŒæ—¶å‘é€å’Œæ¥æ”¶
- åˆ†ä¸º Reduce-Scatter å’Œ All-Gather ä¸¤é˜¶æ®µ
- é€šä¿¡é‡ = `2 Ã— (N-1)/N Ã— data_size`

### 2.3 AllToAll æ¨¡å‹

ç”¨äº **EPï¼ˆä¸“å®¶å¹¶è¡Œï¼‰** çš„ MoE dispatch/combineï¼š

```python
class AllToAllModel:
    def predict(self, data_size_bytes, num_gpus, ...):
        # dispatch + combine ä¸¤æ¬¡é€šä¿¡
        a2a_factor = (num_gpus - 1) / num_gpus
        dispatch_volume = data_size_bytes * a2a_factor
        combine_volume = data_size_bytes * a2a_factor
        
        # åˆ†æ®µæ‹Ÿåˆ
        if single_a2a_mb < threshold:
            # å°æ¶ˆæ¯ï¼šå¹‚å¾‹æ¨¡å‹ï¼ˆå»¶è¿Ÿä¸»å¯¼ï¼‰
            transfer_ms = alpha * (size ** beta)
        else:
            # å¤§æ¶ˆæ¯ï¼šçº¿æ€§æ¨¡å‹ï¼ˆå¸¦å®½ä¸»å¯¼ï¼‰
            transfer_ms = data_size_gb / bandwidth_gbs * 1000
        
        # è€ƒè™‘è´Ÿè½½ä¸å‡è¡¡
        total_time = (latency_ms + transfer_ms) * imbalance_factor
```

**å…³é”®ä¿®å¤**ï¼šNCCL AllToAll æ˜¯**å¹¶è¡Œé€šä¿¡**ï¼Œä¸éœ€è¦ä¹˜ä»¥ `num_messages`ï¼

### 2.4 P2P æ¨¡å‹

ç”¨äº **PPï¼ˆæµæ°´çº¿å¹¶è¡Œï¼‰** çš„ Send/Recvï¼š

```python
class P2PModel:
    def predict(self, data_size_bytes, is_intra_node):
        # ç®€å•çš„å»¶è¿Ÿ + å¸¦å®½æ¨¡å‹
        return latency_us / 1000 + data_size_gb / bandwidth_gbs * 1000
```

### 2.5 ç»Ÿä¸€é€šä¿¡æ¨¡å‹

```python
class CommunicationModel:
    def predict_tp_comm(self, activation_size, tp_degree):
        # TP AllReduceï¼Œé€šå¸¸åœ¨èŠ‚ç‚¹å†…
        is_intra_node = tp_degree <= gpus_per_node
        return self.allreduce.predict(activation_size, tp_degree, is_intra_node)
    
    def predict_ep_comm(self, token_data, ep_degree, topk, num_experts):
        # EP AllToAllï¼Œå¯èƒ½è·¨èŠ‚ç‚¹
        return self.alltoall.predict(token_data, ep_degree, ...)
    
    def predict_pp_comm(self, activation_size, pp_degree, num_micro_batches):
        # PP P2Pï¼Œé€šå¸¸è·¨èŠ‚ç‚¹
        return self.p2p.predict(activation_size, False) * num_comms
```

---

## 3ï¸âƒ£ `computation_model.py` - è®¡ç®—æ¨¡å‹æ¨¡å—

### 3.1 Î±-Î² æ¨¡å‹

è®¡ç®—æ—¶é—´ = Î± Ã— batch_size + Î²

```python
class AlphaBetaModel:
    def fit(self, batch_sizes, times_ms, key):
        # çº¿æ€§å›å½’æ‹Ÿåˆ
        alpha = (n*Î£xy - Î£x*Î£y) / (n*Î£xÂ² - (Î£x)Â²)
        beta = (Î£y - Î±*Î£x) / n
        
    def predict(self, batch_size, key):
        return alpha * batch_size + beta
```

### 3.2 å±‚è®¡ç®—æ—¶é—´ä¼°ç®—

**Attention å±‚**ï¼š
```python
# FLOPs = 4 Ã— hÂ² (Q,K,V,O projections) + 2 Ã— seqÂ² Ã— h (attention scores)
attention_flops = 4 * h * h
time = flops / (peak_tflops * efficiency)
```

**MLP å±‚**ï¼š
```python
# SwiGLU: Gate + Up + Down = 8 Ã— h Ã— ffn
mlp_flops = 8 * h * intermediate_size
```

**MoE å±‚**ï¼š
```python
def estimate_moe_time(self, batch_size, seq_len, tp_degree, ep_degree):
    # Router: å¯¹æ‰€æœ‰ tokens è®¡ç®—
    router_time = self.estimate_layer_time(MOE_ROUTER, ...)
    
    # Expert: æ¯ä¸ª token é€‰ topk ä¸ª experts
    # æ€»è®¡ç®—é‡ = base_time * topk / ep_degree
    workload_factor = topk / ep_degree  # å…³é”®ä¿®å¤ï¼
    expert_time = base_expert_time * workload_factor
    
    return router_time + expert_time
```

**å…³é”®ä¿®å¤**ï¼šMoE Expert è®¡ç®—é‡ = `topk / ep_degree`ï¼Œä¹‹å‰æ˜¯ `topk / ep_degree / ep_degree`ï¼ˆåŒé‡é™¤æ³•é”™è¯¯ï¼‰

### 3.3 æµæ°´çº¿æ°”æ³¡æ—¶é—´

```python
def estimate_pipeline_bubble_time(self, forward_time, backward_time, pp_degree, num_micro_batches):
    # 1F1B è°ƒåº¦
    bubble_ratio = (pp_degree - 1) / num_micro_batches
    return (forward_time + backward_time) * bubble_ratio
```

---

## 4ï¸âƒ£ `memory_model.py` - æ˜¾å­˜æ¨¡å‹æ¨¡å—

### 4.1 æ˜¾å­˜ç»„æˆ

```
æ€»æ˜¾å­˜ = å‚æ•° + æ¢¯åº¦ + ä¼˜åŒ–å™¨çŠ¶æ€ + æ¿€æ´»å€¼ + é€šä¿¡ç¼“å†²åŒº + ä¸´æ—¶ç¼“å†²åŒº + CUDAä¿ç•™
```

### 4.2 å‚æ•°æ˜¾å­˜

```python
def estimate_parameter_count(self, parallel):
    # Attention: 4 * hÂ² (Q,K,V,O) / tp
    attention_params = 4 * h * h // tp
    
    # MLP: 3 * h * ffn (Gate + Up + Down) / tp
    mlp_params = 3 * h * ffn // tp
    
    # MoE Experts: 3 * h * ffn * (num_experts / ep) / tp
    expert_params = 3 * h * ffn * experts_per_gpu // tp
```

### 4.3 ä¼˜åŒ–å™¨çŠ¶æ€æ˜¾å­˜

```python
def estimate_optimizer_memory(self, parallel, zero_config, offload_config):
    # AdamW: 2 Ã— fp32 states (momentum + variance)
    # ZeRO-1/2/3: åˆ†ç‰‡åˆ° DP ç»„
    sharding_factor = zero_config.get_optimizer_sharding_factor(dp_degree)
    optimizer_bytes = total_params * 4 * 2 / sharding_factor
```

### 4.4 æ¿€æ´»å€¼æ˜¾å­˜

```python
def estimate_activation_memory(self, parallel, checkpoint_config, offload_config):
    if self.training.use_flash_attention:
        # FlashAttention: ä¸å­˜å‚¨å®Œæ•´ seqÃ—seq attention scores
        attention_activation = (
            3 * bsz * seq * h +  # Q, K, V
            bsz * seq * h +      # output
            2 * bsz * num_heads * seq  # softmax statistics
        )
    else:
        # æ ‡å‡† Attention: éœ€è¦å­˜å‚¨ seqÃ—seq çŸ©é˜µ
        attention_activation = (
            3 * bsz * seq * h +
            2 * bsz * num_heads * seq * seq +  # å…³é”®å·®å¼‚ï¼
            bsz * seq * h
        )
```

**å…³é”®ä¿®å¤**ï¼šæ·»åŠ  `use_flash_attention` é…ç½®ï¼ŒFlashAttention æ¨¡å¼ä¸‹æ¿€æ´»å€¼æ˜¾å­˜ä¸ `seq_len` **çº¿æ€§ç›¸å…³**ï¼Œè€Œéå¹³æ–¹ï¼

### 4.5 è‡ªåŠ¨æ˜¾å­˜ä¼˜åŒ–å™¨

```python
class AutoMemoryOptimizer:
    def find_optimal_config(self, parallel):
        # æŒ‰è®¡ç®—å¼€é”€ä»å°åˆ°å¤§å°è¯•
        strategies = [
            (ZeRO.NONE, CHECKPOINT.NONE, offload=False),    # æœ€å¿«
            (ZeRO.STAGE_1, CHECKPOINT.NONE, False),         # ä¼˜åŒ–å™¨åˆ†ç‰‡
            (ZeRO.STAGE_2, CHECKPOINT.SELECTIVE, False),    # + æ¢¯åº¦åˆ†ç‰‡ + é€‰æ‹©æ€§é‡è®¡ç®—
            (ZeRO.STAGE_3, CHECKPOINT.FULL, True),          # å…¨é‡ä¼˜åŒ– + offload
        ]
        
        for strategy in strategies:
            if fits_memory(strategy):
                return strategy
```

---

## 5ï¸âƒ£ `integrated_costmodel.py` - é›†æˆæ¨¡å‹

### 5.1 å®Œæ•´é¢„æµ‹æµç¨‹

```python
class GalvatronCostModel:
    def predict_full(self, parallel, micro_batch_size, sequence_length):
        # 1. è‡ªåŠ¨é€‰æ‹©ä¼˜åŒ–ç­–ç•¥
        zero_cfg, ckpt_cfg, offload_cfg = self.memory_optimizer.find_optimal_config(parallel)
        
        # 2. æ˜¾å­˜é¢„æµ‹
        memory = self.memory_model.estimate_memory(parallel, zero_cfg, ckpt_cfg, offload_cfg)
        
        # 3. æ—¶å»¶é¢„æµ‹
        compute_time = (forward_time + backward_time) * num_micro_batches + bubble_time
        comm_time = tp_comm + ep_comm + pp_comm + dp_comm * (1 - overlap_factor)
        total_time = compute_time + comm_time
        
        # 4. æ•ˆç‡æŒ‡æ ‡
        mfu = actual_tflops / peak_tflops
```

### 5.2 é…ç½®æ’åº

```python
def rank_configurations(self, configs, top_k=10):
    results = []
    for cfg in configs:
        prediction = self.predict_full(ParallelConfig(**cfg), ...)
        results.append({
            "config": cfg,
            "total_step_time_ms": prediction.total_step_time_ms,
            "total_memory_gb": prediction.memory_breakdown.total_memory_gb,
            "fits_memory": prediction.fits_memory,
            "mfu": prediction.mfu,
        })
    
    # æ’åºï¼šå…ˆæŒ‰ fits_memoryï¼Œå†æŒ‰æ—¶å»¶
    results.sort(key=lambda x: (not x["fits_memory"], x["total_step_time_ms"]))
```

---

## ğŸ“Š å•ä½çº¦å®šæ€»ç»“

| ç‰©ç†é‡ | å•ä½ | ç¤ºä¾‹ |
|-------|------|------|
| ç®—åŠ› | TFLOPS (10^12 FLOPS) | H800: 989 TFLOPS |
| æ˜¾å­˜å®¹é‡ | GB | 80 GB |
| å¸¦å®½ | **GB/s** (å­—èŠ‚/ç§’) | NVLink: 900 GB/s, IB: 25 GB/s |
| å»¶è¿Ÿ | **Î¼s** (å¾®ç§’) | NVLink: 1 Î¼s, IB: 5 Î¼s |
| æ—¶é—´ | **ms** (æ¯«ç§’) | step time: 1000 ms |
| æ•°æ®é‡ | **bytes** | 10 MB = 10Ã—1024Â² bytes |

---

## ğŸ”§ å…³é”®ä¿®å¤å›é¡¾

1. **AllToAll é€šä¿¡é‡ä¿®å¤**ï¼šä»ç®€å• `Ã—2` æ”¹ä¸ºæ­£ç¡®çš„ `dispatch + combine` åˆ†å¼€è®¡ç®—
2. **AllToAll å¹¶è¡Œæ¨¡å‹ä¿®å¤**ï¼šNCCL AllToAll æ˜¯å¹¶è¡Œé€šä¿¡ï¼Œä¸éœ€è¦ä¹˜ä»¥ `num_messages`
3. **MoE Expert è®¡ç®—é‡ä¿®å¤**ï¼š`workload_factor = topk / ep`ï¼Œç§»é™¤åŒé‡é™¤æ³•
4. **FlashAttention æ”¯æŒ**ï¼šæ·»åŠ  `use_flash_attention` é…ç½®ï¼Œæ­£ç¡®ä¼°ç®—æ¿€æ´»å€¼æ˜¾å­˜
5. **å•ä½ç»Ÿä¸€**ï¼šæ‰€æœ‰å¸¦å®½ç»Ÿä¸€ä¸º GB/sï¼Œç§»é™¤ `/8.0` è½¬æ¢é€»è¾‘

è¿™äº›ä¿®å¤ç¡®ä¿äº† CostModel åœ¨é¢„æµ‹å¤§è§„æ¨¡ MoE æ¨¡å‹è®­ç»ƒæ—¶çš„å‡†ç¡®æ€§ã€‚

---

è®²è§£å®Œæˆï¼å¦‚æœä½ å¯¹æŸä¸ªå…·ä½“éƒ¨åˆ†è¿˜æœ‰ç–‘é—®ï¼Œå¯ä»¥ç»§ç»­é—®æˆ‘ã€‚
ReportID: 7170e20d-a929-47b2-ac32-60f2fba6a822
ConversationID: 5ab75f76-b797-42f6-9f72-3739182a3702
